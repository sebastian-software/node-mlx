//
//  Bloom.swift
//  Auto-generated by hf2swift v3
//
//  Manual review required before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - Model Components

// MARK: - BloomGelu

class BloomGelu: Module {

    init(_ config: BloomConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        return GeLUFunction.apply(x)
    }
}

// MARK: - BloomAttention

class BloomAttention: Module {
    @ModuleInfo(key: "query_key_value") var queryKeyValue: Linear
    @ModuleInfo(key: "dense") var dense: Linear

    init(_ config: BloomConfiguration) {
        self._queryKeyValue.wrappedValue = Linear(hiddenSize, 3 * hiddenSize, bias: true)
        self._dense.wrappedValue = Linear(hiddenSize, hiddenSize)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ residual: MLXArray, _ alibi: MLXArray, _ attentionMask: MLXArray, _ layerPast: MLXArray, _ useCache: MLXArray, _ outputAttentions: MLXArray, _ cachePosition: MLXArray) -> MLXArray {
        let (batchSize, QLength, ) = hiddenStates.shape
        let fusedQkv = queryKeyValue(hiddenStates)
        let (queryLayer, KeyLayer, ValueLayer) = _reshape(fusedQkv)
        // if layerPast is not nil { ... }
        let queryLayer = queryLayer.reshaped(batchSize * numHeads, -1, headDim)
        let keyLayer = keyLayer.reshaped(batchSize * numHeads, -1, headDim).transpose(-1, -2)
        let valueLayer = valueLayer.reshaped(batchSize * numHeads, -1, headDim)
        let attentionScores = alibi.baddbmm(batch1=queryLayer, batch2=keyLayer, beta=beta, alpha=invNormFactor)
        let attnWeights = attentionScores.reshaped(batchSize, numHeads, qLength, -1)
        // if attentionMask is not nil { ... }
        let attentionProbs = F.softmax(attnWeights, dim=-1, dtype=torch.float32)
        let attentionProbs = attentionDropout(attentionProbs)
        let attentionProbsReshaped = attentionProbs.reshaped(batchSize * numHeads, qLength, -1)
        let contextLayer = torch.bmm(attentionProbsReshaped, valueLayer)
        let contextLayer = _merge_heads(contextLayer)
        // if pretrainingTp > 1 and slowButExact { ... }
        let outputTensor = dropoutAdd(outputTensor, residual, hiddenDropout, training)
        return (outputTensor, attentionProbs)
    }
}

// MARK: - BloomMLP

class BloomMLP: Module {
    @ModuleInfo(key: "dense_h_to_4h") var denseHTo4H: Linear
    @ModuleInfo(key: "dense_4h_to_h") var dense4HToH: Linear

    init(_ config: BloomConfiguration) {
        self._denseHTo4H.wrappedValue = Linear(hiddenSize, 4 * hiddenSize)
        self._dense4HToH.wrappedValue = Linear(4 * hiddenSize, hiddenSize)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ residual: MLXArray) -> MLXArray {
        let hiddenStates = geluImpl(dense_h_to_4h(hiddenStates))
        // if pretrainingTp > 1 and slowButExact { ... }
        let output = dropoutAdd(intermediateOutput, residual, hiddenDropout, training)
        return output
    }
}
