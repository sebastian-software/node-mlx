//
//  GptOss.swift
//  Auto-generated by hf2swift v3
//
//  Manual review required before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - Model Components

// MARK: - GptOssRMSNorm

class GptOssRMSNorm: Module {

    init(_ config: GptOssConfiguration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let inputDtype = hiddenStates.dtype
        let hiddenStates = hiddenStates
        let variance = hiddenStates.pow(2).mean(-1, keepdim=true)
        let hiddenStates = hiddenStates * rsqrt(variance + varianceEpsilon)
        return (weight * hiddenStates)
    }
}

// MARK: - GptOssExperts

class GptOssExperts: Module {

    init(_ config: GptOssConfiguration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ routerIndices: MLXArray, _ routingWeights: MLXArray) -> MLXArray {
        let batchSize = hiddenStates.dim(0)
        let hiddenStates = hiddenStates.reshaped(-1, hiddenSize)
        // if hiddenStates.device.type == 'cpu' or training { ... }
        return nextStates
    }
}

// MARK: - GptOssTopKRouter

class GptOssTopKRouter: Module {

    init(_ config: GptOssConfiguration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let hiddenStates = hiddenStates.reshaped(-1, hiddenDim)
        let routerLogits = F.linear(hiddenStates, weight, bias)
        let (routerTopValue, RouterIndices) = torch.topk(routerLogits, topK, dim=-1)
        let routerTopValue = torch.nn.functional.softmax(routerTopValue, dim=1, dtype=routerTopValue.dtype)
        let routerScores = routerTopValue
        return (routerLogits, routerScores, routerIndices)
    }
}

// MARK: - GptOssMLP

class GptOssMLP: Module {

    init(_ config: GptOssConfiguration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let (, RouterScores, RouterIndices) = router(hiddenStates)
        let routedOut = experts(hiddenStates, routerIndices, routerScores)
        return (routedOut, routerScores)
    }
}

// MARK: - GptOssRotaryEmbedding

class GptOssRotaryEmbedding: Module {

    init(_ config: GptOssConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray) -> MLXArray {
        let invFreqExpanded = invFreq[nil, :, nil].asType(.float32).expand(positionIds.dim(0), -1, 1)
        let positionIdsExpanded = positionIds[:, nil, :].asType(.float32)
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with block (skipped)
        return (cos, sin)
    }
}

// MARK: - GptOssAttention

class GptOssAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear

    init(_ config: GptOssConfiguration) {
        self._qProj.wrappedValue = Linear(config.hiddenSize, config.numAttentionHeads * headDim, bias: config.attentionBias)
        self._kProj.wrappedValue = Linear(config.hiddenSize, config.numKeyValueHeads * headDim, bias: config.attentionBias)
        self._vProj.wrappedValue = Linear(config.hiddenSize, config.numKeyValueHeads * headDim, bias: config.attentionBias)
        self._oProj.wrappedValue = Linear(config.numAttentionHeads * headDim, config.hiddenSize, bias: config.attentionBias)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ positionEmbeddings: MLXArray, _ attentionMask: MLXArray, _ pastKeyValues: MLXArray, _ cachePosition: MLXArray) -> MLXArray {
        let inputShape = hiddenStates.shape[:-1]
        let hiddenShape = (*inputShape, -1, headDim)
        let queryStates = qProj(hiddenStates).reshaped(hiddenShape).transposed(1, 2)
        let keyStates = kProj(hiddenStates).reshaped(hiddenShape).transposed(1, 2)
        let valueStates = vProj(hiddenStates).reshaped(hiddenShape).transposed(1, 2)
        let (cos, sin) = positionEmbeddings
        let (queryStates, KeyStates) = applyRotaryPosEmb(queryStates, keyStates, cos, sin)
        // if pastKeyValues is not nil { ... }
        // attention_interface: Callable = eager_attention_forward...
        // if config._attn_implementation != 'eager' { ... }
        let (attnOutput, AttnWeights) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=0.0 if not training else attentionDropout, scaling=scaling, slidingWindow=slidingWindow, sAux=sinks, **kwargs)
        let attnOutput = attnOutput.reshaped(*inputShape, -1)
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }
}
