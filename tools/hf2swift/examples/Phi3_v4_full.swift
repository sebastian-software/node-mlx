//
//  Phi3.swift
//  Auto-generated by hf2swift v4
//
//  Review and adjust before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN

// MARK: - Configuration

public struct Phi3Configuration: Codable, Sendable {
    public var attentionBias: Bool
    public let attentionDropout: Float?
    public var bosTokenId: Int
    public let embdPdrop: Float?
    public var eosTokenId: Int
    public var hiddenAct: String
    public var hiddenSize: Int
    public let initializerRange: Float?
    public var intermediateSize: Int
    public var maxPositionEmbeddings: Int
    public var modelType: String
    public var numAttentionHeads: Int
    public var numHiddenLayers: Int
    public var numKeyValueHeads: Int
    public let originalMaxPositionEmbeddings: Int?
    public let padTokenId: Int?
    public let residPdrop: Float?
    public var rmsNormEps: Float
    public let ropeScaling: Any?
    public var ropeTheta: Int
    public let slidingWindow: Any?
    public var tieWordEmbeddings: Bool
    public let useCache: Bool?
    public var vocabSize: Int

    enum CodingKeys: String, CodingKey {
        case attentionBias = "attention_bias"
        case attentionDropout = "attention_dropout"
        case bosTokenId = "bos_token_id"
        case embdPdrop = "embd_pdrop"
        case eosTokenId = "eos_token_id"
        case hiddenAct = "hidden_act"
        case hiddenSize = "hidden_size"
        case initializerRange = "initializer_range"
        case intermediateSize = "intermediate_size"
        case maxPositionEmbeddings = "max_position_embeddings"
        case modelType = "model_type"
        case numAttentionHeads = "num_attention_heads"
        case numHiddenLayers = "num_hidden_layers"
        case numKeyValueHeads = "num_key_value_heads"
        case originalMaxPositionEmbeddings = "original_max_position_embeddings"
        case padTokenId = "pad_token_id"
        case residPdrop = "resid_pdrop"
        case rmsNormEps = "rms_norm_eps"
        case ropeScaling = "rope_scaling"
        case ropeTheta = "rope_theta"
        case slidingWindow = "sliding_window"
        case tieWordEmbeddings = "tie_word_embeddings"
        case useCache = "use_cache"
        case vocabSize = "vocab_size"
    }

    // Computed properties
    public var headDim: Int {
        hiddenSize / numAttentionHeads
    }
}

// MARK: - Helper Functions

/// Apply rotary position embeddings
func applyRotaryPosEmb(
    _ q: MLXArray,
    _ k: MLXArray,
    _ cos: MLXArray,
    _ sin: MLXArray
) -> (MLXArray, MLXArray) {
    let qEmbed = (q * cos) + (rotateHalf(q) * sin)
    let kEmbed = (k * cos) + (rotateHalf(k) * sin)
    return (qEmbed, kEmbed)
}

/// Rotate half of the hidden dims
func rotateHalf(_ x: MLXArray) -> MLXArray {
    let half = x.dim(-1) / 2
    let x1 = x[..., ..<half]
    let x2 = x[..., half...]
    return concatenated([-x2, x1], axis: -1)
}

// MARK: - Model Components

// MARK: - Phi3MLP

class Phi3MLP: Module {
    @ModuleInfo(key: "gate_up_proj") var gateUpProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: Phi3Configuration) {

        self._gateUpProj.wrappedValue = Linear(config.hiddenSize, 2 * config.intermediateSize, bias: false)
        self._downProj.wrappedValue = Linear(config.intermediateSize, config.hiddenSize, bias: false)
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let upStates = gateUpProj(hiddenStates)
        let ((gate, upStates)) = upStates.split(parts: 2, axis: -1)
        let upStates = upStates * activationFn(gate)
        return downProj(upStates)
    }

}

// MARK: - Phi3RotaryEmbedding

class Phi3RotaryEmbedding: Module {

    init(_ config: Phi3Configuration) {

    }

    func computeDefaultRopeParameters(_ device: MLXArray, _ seqLen: MLXArray) -> MLXArray {
        let base = config.ropeParameters['ropeTheta']
        let partialRotaryFactor = config.ropeParameters.get('partialRotaryFactor', 1.0)
        let headDim = getattr(config, 'headDim', nil) or config.hiddenSize // config.numAttentionHeads
        let dim = int(headDim * partialRotaryFactor)
        let attentionFactor = 1.0
        let invFreq = 1.0 / base ** (torch.arange(0, dim, 2, dtype=torch.int64) / dim)
        return (invFreq, attentionFactor)
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray) -> MLXArray {
        let invFreqExpanded = invFreq[nil, :, nil].asType(.float32).expand(positionIds.dim(0), -1, 1)
        let positionIdsExpanded = positionIds[:, nil, :].asType(.float32)
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with block - may need manual conversion
        let freqs = (invFreqExpanded.asType(.float32) @ positionIdsExpanded.asType(.float32)).transposed(1, 2)
        let emb = concatenated([freqs, freqs], axis: -1)
        let cos = emb.cos() * attentionScaling
        let sin = emb.sin() * attentionScaling
        return (cos, sin)
    }

}

// MARK: - Phi3Attention

class Phi3Attention: Module {
    let numKeyValueGroups: Int
    let scaling: Int
    let isCausal: Int
    @ModuleInfo(key: "o_proj") var oProj: Linear
    @ModuleInfo(key: "qkv_proj") var qkvProj: Linear

    init(_ config: Phi3Configuration) {
        self.numKeyValueGroups = config.numAttentionHeads // config.numKeyValueHeads
        self.scaling = headDim ** (-0.5)
        self.isCausal = true

        self._oProj.wrappedValue = Linear(config.numAttentionHeads * headDim, config.hiddenSize, bias: false)
        self._qkvProj.wrappedValue = Linear(config.hiddenSize, opSize, bias: false)
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ positionEmbeddings: MLXArray, _ attentionMask: MLXArray?, _ pastKeyValues: MLXArray?, _ cachePosition: MLXArray?) -> MLXArray {
        let inputShape = hiddenStates.shape.dropLast()
        let hiddenShape = (*inputShape, -1, headDim)
        let qkv = qkvProj(hiddenStates)
        let queryPos = config.numAttentionHeads * headDim
        let queryStates = qkv[..., :queryPos]
        let keyStates = qkv[..., queryPos:queryPos + numKeyValueHeads * headDim]
        let valueStates = qkv[..., queryPos + numKeyValueHeads * headDim:]
        let queryStates = queryStates.reshaped([hiddenShape]).transposed(1, 2)
        let keyStates = keyStates.reshaped([hiddenShape]).transposed(1, 2)
        let valueStates = valueStates.reshaped([hiddenShape]).transposed(1, 2)
        let ((cos, sin)) = positionEmbeddings
        let ((queryStates, keyStates)) = applyRotaryPosEmb(queryStates, keyStates, cos, sin)
        if pastKeyValues is not nil {
            let cacheKwargs = {'sin': sin, 'cos': cos, 'cachePosition': cachePosition}
            let ((keyStates, valueStates)) = pastKeyValues.update(keyStates, valueStates, layerIdx, cacheKwargs)
        }
        // TODO: attention_interface: Callable = eager_attention_forward
        if config._attn_implementation != 'eager' {
            let attentionInterface = ALL_ATTENTION_FUNCTIONS[config._attn_implementation]
        }
        let ((attnOutput, attnWeights)) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=0.0 if not training else attentionDropout, scaling=scaling, slidingWindow=getattr(config, 'slidingWindow', nil), **kwargs)
        let attnOutput = attnOutput.reshaped([*inputShape, -1])
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }

}

// MARK: - Phi3RMSNorm

class Phi3RMSNorm: Module {

    init(_ config: Phi3Configuration) {

    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let inputDtype = hiddenStates.dtype
        let hiddenStates = hiddenStates
        let variance = hiddenStates.power(2).mean(axis: -1)
        let hiddenStates = hiddenStates * rsqrt(variance + varianceEpsilon)
        return weight * hiddenStates
    }

    func extraRepr() -> MLXArray {
        return f'{tuple(weight.shape)}, eps={varianceEpsilon}'
    }

}

// MARK: - Phi3TransformerBlock

class Phi3TransformerBlock: Module {
    @ModuleInfo(key: "self_attn") var attention: Phi3Attention
    let mlp: Phi3MLP
    @ModuleInfo(key: "input_layernorm") var inputLayerNorm: RMSNorm
    @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayerNorm: RMSNorm

    init(_ config: Phi3Configuration) {
        self._attention.wrappedValue = Phi3Attention(config)
        self.mlp = Phi3MLP(config)
        self._inputLayerNorm.wrappedValue = RMSNorm(
            dimensions: config.hiddenSize, eps: config.rmsNormEps)
        self._postAttentionLayerNorm.wrappedValue = RMSNorm(
            dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    func callAsFunction(
        _ x: MLXArray,
        mask: MLXArray? = nil,
        cache: KVCache? = nil
    ) -> MLXArray {
        // Self-attention with residual
        let residual = x
        let h = inputLayerNorm(x)
        let attnOut = attention(h, mask: mask, cache: cache)
        let h = residual + attnOut

        // MLP with residual
        let residual = h
        let h = postAttentionLayerNorm(h)
        let mlpOut = mlp(h)
        return residual + mlpOut
    }
}

// MARK: - Phi3ModelInner

public class Phi3ModelInner: Module {
    @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
    let layers: [Phi3TransformerBlock]
    @ModuleInfo(key: "norm") var norm: RMSNorm

    public init(_ config: Phi3Configuration) {
        self._embedTokens.wrappedValue = Embedding(
            embeddingCount: config.vocabSize, dimensions: config.hiddenSize)

        self.layers = (0..<config.numHiddenLayers).map { _ in
            Phi3TransformerBlock(config)
        }

        self._norm.wrappedValue = RMSNorm(
            dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
        var h = embedTokens(inputs)

        for (i, layer) in layers.enumerated() {
            h = layer(h, mask: nil, cache: cache?[i])
        }

        return norm(h)
    }
}

// MARK: - Phi3Model

public class Phi3Model: Module, LLMModel {
    public let vocabularySize: Int
    public let kvHeads: [Int]

    public let model: Phi3ModelInner
    private let config: Phi3Configuration

    @ModuleInfo(key: "lm_head") var lmHead: Linear?

    public init(_ config: Phi3Configuration) {
        self.vocabularySize = config.vocabSize
        self.kvHeads = (0..<config.numHiddenLayers).map { _ in config.numKeyValueHeads }
        self.model = Phi3ModelInner(config)
        self.config = config

        if !config.tieWordEmbeddings {
            self._lmHead.wrappedValue = Linear(
                config.hiddenSize, config.vocabSize, bias: false)
        }
    }

    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {
        let out = model(inputs, cache: cache)
        if config.tieWordEmbeddings {
            return model.embedTokens.asLinear(out)
        } else if let lmHead {
            return lmHead(out)
        } else {
            fatalError("Model config error: No lm_head or tied embeddings")
        }
    }

    // MARK: - LLMModel Protocol

    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
        weights  // Override if weight key mapping needed
    }
}