//
//  Stablelm.swift
//  Auto-generated by hf2swift v3
//
//  Manual review required before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - Model Components

// MARK: - StableLmRotaryEmbedding

class StableLmRotaryEmbedding: Module {

    init(_ config: StablelmConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray) -> MLXArray {
        let invFreqExpanded = invFreq[nil, :, nil].asType(.float32).expand(positionIds.dim(0), -1, 1)
        let positionIdsExpanded = positionIds[:, nil, :].asType(.float32)
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with block (skipped)
        return (cos, sin)
    }
}

// MARK: - StableLmMLP

class StableLmMLP: Module {
    @ModuleInfo(key: "gate_proj") var gateProj: Linear
    @ModuleInfo(key: "up_proj") var upProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: StablelmConfiguration) {
        self._gateProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)
        self._upProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)
        self._downProj.wrappedValue = Linear(intermediateSize, hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let downProj = downProj(actFn(gateProj(x)) * upProj(x))
        return downProj
    }
}

// MARK: - StableLmLayerNormPerHead

class StableLmLayerNormPerHead: Module {
    @ModuleInfo(key: "norms") var norms: Array

    init(_ config: StablelmConfiguration) {
        self._norms.wrappedValue = Array([nn.LayerNorm(dim, eps=eps, bias=bias) for _ in range(numHeads)])
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let statesPerHeads = torch.split(hiddenStates, 1, dim=1)
        return concatenated([norm(hiddenStates) for norm, hiddenStates in zip(norms, statesPerHeads)], axis: 1)
    }
}

// MARK: - StableLmAttention

class StableLmAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear

    init(_ config: StablelmConfiguration) {
        self._qProj.wrappedValue = Linear(hiddenSize, numHeads * headDim, bias: config.useQkvBias)
        self._kProj.wrappedValue = Linear(hiddenSize, numKeyValueHeads * headDim, bias: config.useQkvBias)
        self._vProj.wrappedValue = Linear(hiddenSize, numKeyValueHeads * headDim, bias: config.useQkvBias)
        self._oProj.wrappedValue = Linear(hiddenSize, hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ attentionMask: MLXArray, _ positionIds: MLXArray, _ pastKeyValues: MLXArray, _ outputAttentions: MLXArray, _ useCache: MLXArray, _ cachePosition: MLXArray, _ positionEmbeddings: MLXArray) -> MLXArray {
        let (bsz, qLen, ) = hiddenStates.size()
        let queryStates = qProj(hiddenStates)
        let keyStates = kProj(hiddenStates)
        let valueStates = vProj(hiddenStates)
        let queryStates = queryStates.reshaped(bsz, qLen, numHeads, headDim).transposed(1, 2)
        let keyStates = keyStates.reshaped(bsz, qLen, numKeyValueHeads, headDim).transposed(1, 2)
        let valueStates = valueStates.reshaped(bsz, qLen, numKeyValueHeads, headDim).transposed(1, 2)
        // if qkLayernorm { ... }
        let (cos, sin) = positionEmbeddings
        let (queryRot, QueryPass) = (queryStates[..., :rotaryNdims], queryStates[..., rotaryNdims:])
        let (keyRot, KeyPass) = (keyStates[..., :rotaryNdims], keyStates[..., rotaryNdims:])
        let (queryRot, KeyRot) = applyRotaryPosEmb(queryRot, keyRot, cos, sin)
        let queryStates = torch.cat((queryRot, queryPass), dim=-1)
        let keyStates = torch.cat((keyRot, keyPass), dim=-1)
        // if pastKeyValues is not nil { ... }
        // attention_interface: Callable = eager_attention_forward...
        // if config._attn_implementation != 'eager' { ... }
        let (attnOutput, AttnWeights) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=0.0 if not training else attentionDropout, scaling=scaling, positionIds=positionIds, **kwargs)
        let attnOutput = attnOutput.reshaped(bsz, qLen, -1)
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }
}
