//
//  Gemma.swift
//  Auto-generated by hf2swift v2
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - GemmaRMSNorm

class GemmaRMSNorm: Module {
    let weight: Any

    init(_ config: GemmaConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let output = _norm(x.asType(.float32))
        let output = output * (1.0 + weight.asType(.float32))
        return output.asType(x.dtype)
    }

    func extraRepr() -> MLXArray {
        // return f'{tuple(self.weight.shape)}, eps={self.eps}'
        fatalError("Not fully implemented")
    }

}

// MARK: - GemmaMLP

class GemmaMLP: Module {
    @ModuleInfo(key: "gate_proj") var gateProj: Linear
    @ModuleInfo(key: "up_proj") var upProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: GemmaConfiguration) {
        self._gateProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        self._upProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        self._downProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        super.init()
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let downProj = downProj(actFn(gateProj(x)) * upProj(x))
        return downProj
    }

}

// MARK: - GemmaRotaryEmbedding

class GemmaRotaryEmbedding: Module {

    init(_ config: GemmaConfiguration) {
        super.init()
    }

    func computeDefaultRopeParameters(_ device: MLXArray, _ seqLen: MLXArray) -> MLXArray {
        // '\n        Computes the inverse frequencies according to the original RoPE implementation\n        Args:\n            config ([`~transformers.PreTrainedConfig`]):\n                The model configuration.\n            device (`torch.device`):\n                The device to use for initialization of the inverse frequencies.\n            seq_len (`int`, *optional*):\n                The current sequence length. Unused for this type of RoPE.\n        Returns:\n            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n        '
        // base = config.rope_parameters['rope_theta']
        // dim = getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads
        // attention_factor = 1.0
        // inv_freq = 1.0 / base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)
        // return (inv_freq, attention_factor)
        fatalError("Not fully implemented")
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray) -> MLXArray {
        let invFreqExpanded = invFreq[None, :, None].float().expand(positionIds.dim(0), -1, 1).to(x.device)
        let positionIdsExpanded = positionIds[:, None, :].float()
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with maybe_autocast(device_type=device_type, enabled=False):
    freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
    emb = torch.cat((freqs, freqs), dim=-1)
    cos = emb.cos() * self.attention_scaling
    sin = emb.sin() * self.attention_scaling
        return (cos
    }

}

// MARK: - GemmaAttention

class GemmaAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear

    let headDim: Any

    init(_ config: GemmaConfiguration) {
        self._qProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        self._kProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        self._vProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        self._oProj.wrappedValue = Linear(
            config.hiddenSize,
            config.hiddenSize,
            bias: false
        )
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ positionEmbeddings: MLXArray, _ attentionMask: MLXArray, _ pastKeyValues: MLXArray, _ cachePosition: MLXArray) -> MLXArray {
        let inputShape = hiddenStates.shape[:-1]
        let hiddenShape = (*inputShape, -1, headDim)
        let queryStates = qProj(hiddenStates).view(hiddenShape).transpose(1, 2)
        let keyStates = kProj(hiddenStates).view(hiddenShape).transpose(1, 2)
        let valueStates = vProj(hiddenStates).view(hiddenShape).transpose(1, 2)
        let (cos, sin) = positionEmbeddings
        let (queryStates, KeyStates) = applyRotaryPosEmb(queryStates, keyStates, cos, sin)
        // if past_key_values is not None:
    cache_kwargs = {'sin': sin, 'cos': cos, 'cache_position': cache_position}
    key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
        // attention_interface: Callable = eager_attention_forward
        // if self.config._attn_implementation != 'eager':
    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
        let (attnOutput, AttnWeights) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=0.0 if not training else attentionDropout, scaling=scaling, **kwargs)
        let attnOutput = attnOutput.reshaped(*inputShape, -1).contiguous()
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }

}

// MARK: - GemmaPreTrainedModel

class GemmaPreTrainedModel: Module {

    init(_ config: GemmaConfiguration) {
        super.init()
    }

}
