//
//  Phi3.swift
//  Auto-generated by hf2swift v3
//
//  Manual review required before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - Model Components

// MARK: - Phi3MLP

class Phi3MLP: Module {
    @ModuleInfo(key: "gate_up_proj") var gateUpProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: Phi3Configuration) {
        self._gateUpProj.wrappedValue = Linear(config.hiddenSize, 2 * config.intermediateSize, bias: false)
        self._downProj.wrappedValue = Linear(config.intermediateSize, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let upStates = gateUpProj(hiddenStates)
        let (gate, upStates) = upStates.split(parts: 2, axis: -1)
        let upStates = upStates * activationFn(gate)
        return downProj(upStates)
    }
}

// MARK: - Phi3RotaryEmbedding

class Phi3RotaryEmbedding: Module {

    init(_ config: Phi3Configuration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray) -> MLXArray {
        let invFreqExpanded = invFreq[nil, :, nil].asType(.float32).expand(positionIds.dim(0), -1, 1)
        let positionIdsExpanded = positionIds[:, nil, :].asType(.float32)
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with block (skipped)
        return (cos, sin)
    }
}

// MARK: - Phi3Attention

class Phi3Attention: Module {
    @ModuleInfo(key: "o_proj") var oProj: Linear
    @ModuleInfo(key: "qkv_proj") var qkvProj: Linear

    init(_ config: Phi3Configuration) {
        self._oProj.wrappedValue = Linear(config.numAttentionHeads * headDim, config.hiddenSize, bias: false)
        self._qkvProj.wrappedValue = Linear(config.hiddenSize, opSize, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ positionEmbeddings: MLXArray, _ attentionMask: MLXArray, _ pastKeyValues: MLXArray, _ cachePosition: MLXArray) -> MLXArray {
        let inputShape = hiddenStates.shape[:-1]
        let hiddenShape = (*inputShape, -1, headDim)
        let qkv = qkvProj(hiddenStates)
        let queryPos = config.numAttentionHeads * headDim
        let queryStates = qkv[..., :queryPos]
        let keyStates = qkv[..., queryPos:queryPos + numKeyValueHeads * headDim]
        let valueStates = qkv[..., queryPos + numKeyValueHeads * headDim:]
        let queryStates = queryStates.reshaped(hiddenShape).transposed(1, 2)
        let keyStates = keyStates.reshaped(hiddenShape).transposed(1, 2)
        let valueStates = valueStates.reshaped(hiddenShape).transposed(1, 2)
        let (cos, sin) = positionEmbeddings
        let (queryStates, KeyStates) = applyRotaryPosEmb(queryStates, keyStates, cos, sin)
        // if pastKeyValues is not nil { ... }
        // attention_interface: Callable = eager_attention_forward...
        // if config._attn_implementation != 'eager' { ... }
        let (attnOutput, AttnWeights) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=0.0 if not training else attentionDropout, scaling=scaling, slidingWindow=getattr(config, 'slidingWindow', nil), **kwargs)
        let attnOutput = attnOutput.reshaped(*inputShape, -1)
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }
}

// MARK: - Phi3RMSNorm

class Phi3RMSNorm: Module {

    init(_ config: Phi3Configuration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let inputDtype = hiddenStates.dtype
        let hiddenStates = hiddenStates
        let variance = hiddenStates.pow(2).mean(-1, keepdim=true)
        let hiddenStates = hiddenStates * rsqrt(variance + varianceEpsilon)
        return weight * hiddenStates
    }
}
