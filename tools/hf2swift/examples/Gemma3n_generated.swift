//
//  Gemma3N.swift
//  Auto-generated by hf2swift v3
//
//  Manual review required before use!
//

import Foundation
import MLX
import MLXFast
import MLXNN
import MLXLMCommon

// MARK: - Model Components

// MARK: - Gemma3nRMSNorm

class Gemma3nRMSNorm: Module {

    init(_ config: Gemma3NConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let output = _norm(x.asType(.float32)) * weight.asType(.float32)
        return output.asType(x.dtype)
    }
}

// MARK: - Gemma3nAudioRelativePositionEmbedding

class Gemma3nAudioRelativePositionEmbedding: Module {
    @ModuleInfo(key: "pos_proj") var posProj: Linear

    init(_ config: Gemma3NConfiguration) {
        self._posProj.wrappedValue = Linear(channels, numHeads * headDim, bias: false)
        super.init()
    }

    func callAsFunction(_ queries: MLXArray, _ keys: MLXArray) -> MLXArray {
        let (batchSize, NumQueryBlocks, QueryBlockSize, NumHeads, HeadDim) = queries.shape
        let (, , KeyContextSize, , ) = keys.shape
        let posIndices = MLXArray(0..<maxBackward, -maxForward - 1, -1, device=queries.device).expandedDimensions(axis: 0)
        let maxSpanPlus1 = posIndices.dim(1)
        let sinEmbTimingSignal = _get_timing_signal_1d_pos(posIndices, dtype=queries.dtype)
        let projectedSinEmb = posProj(sinEmbTimingSignal)
        let sinEmb = projectedSinEmb.reshaped(1, max_span_plus_1, numHeads, headDim).squeezed(axis: 0)
        let queriesP = queries.permute(0, 3, 1, 2, 4)
        let keysPT = keys.permute(0, 3, 1, 4, 2)
        let termAc = matmul(queriesP, keysPT)
        let qPermuted = queries.permute(0, 3, 1, 2, 4)
        let sPermuted = sinEmb.permute(1, 2, 0)
        let qReshaped = qPermuted.reshaped(batchSize, numHeads, numQueryBlocks * queryBlockSize, headDim)
        let termBdUnshifedMatmul = matmul(qReshaped, sPermuted)
        let termBdUnshifed = termBdUnshifedMatmul.reshaped(batchSize, numHeads, numQueryBlocks, queryBlockSize, max_span_plus_1)
        let termBdShifted = _relative_shift(termBdUnshifed, batchSize, numHeads, numQueryBlocks, queryBlockSize, keyContextSize, max_span_plus_1)
        return termAc + termBdShifted
    }
}

// MARK: - Gemma3nAudioAttention

class Gemma3nAudioAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear

    init(_ config: Gemma3NConfiguration) {
        self._qProj.wrappedValue = Linear(hiddenSize, numHeads * headDim, bias: false)
        self._kProj.wrappedValue = Linear(hiddenSize, numHeads * headDim, bias: false)
        self._vProj.wrappedValue = Linear(hiddenSize, numHeads * headDim, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ mask: MLXArray) -> MLXArray {
        let qkvShape = (*hiddenStates.shape[:-1], numHeads, headDim)
        let queryStates = qProj(hiddenStates).reshaped(qkvShape)
        let keyStates = kProj(hiddenStates).reshaped(qkvShape)
        let valueStates = vProj(hiddenStates).reshaped(qkvShape)
        let perDimScaleSp = torch.nn.functional.softplus(perDimScale)
        let broadcastShape = (1, 1, 1, headDim)
        let perDimScaleSpBroadcast = perDimScaleSp.reshaped(broadcastShape)
        let queryStates = queryStates * qScale * perDimScaleSpBroadcast
        let (batchSize, QTime) = queryStates.shape[:2]
        let queryBlocks = _convert_to_block(queryStates)
        let keyBlocks = _extract_block_context(keyStates)
        let valueBlocks = _extract_block_context(valueStates)
        let numQueryBlocks = queryBlocks.dim(1)
        let originalValidMask = ~mask
        let extractedValidMaskBlocks = _extract_block_context(originalValidMask)
        // if extractedValidMaskBlocks.ndim == 4 and extractedValidMaskBlocks.dim(2) * extractedValidMaskBlocks.dim(3) == contextSize { ... }
        // if extractedValidMaskBlocks.shape != (batchSize, numQueryBlocks, contextSize) { ... }
        let conditionFromInputValidity = extractedValidMaskBlocks.expandedDimensions(axis: 1).unsqueeze(-2)
        let conditionFromCausality = localCausalValidMask.expandedDimensions(axis: 0).expandedDimensions(axis: 0).expandedDimensions(axis: 0)
        let finalConditionForWhere = torch.logicalAnd(conditionFromInputValidity, conditionFromCausality)
        let logits = relativePositionEmbedding(queryBlocks, keyBlocks)
        let softcapVal = softcap
        let logits = logits / softcapVal
        let logits = tanh(logits)
        let logits = logits * softcapVal
        let logits = MLX.where(finalConditionForWhere, logits, torch.finfo(logits.dtype).min)
        let probabilities = torch.nn.functional.softmax(logits, dim=-1, dtype=torch.float32)
        let (bDim, NDim, UDim, WDim, CDim) = probabilities.shape
        let hDim = valueBlocks.shape[-1]
        let probBun = probabilities.permute(0, 2, 1, 3, 4).reshaped(-1, wDim, cDim)
        let vBun = valueBlocks.permute(0, 1, 3, 2, 4).reshaped(-1, cDim, hDim)
        let resultBmm = torch.bmm(probBun, vBun)
        let contextVectors = resultBmm.reshaped(bDim, uDim, nDim, wDim, hDim).permute(0, 1, 3, 2, 4)
        let contextVectors = contextVectors.reshaped((batchSize, numQueryBlocks * chunkSize, numHeads, headDim))
        let contextVectors = contextVectors[:, :qTime]
        return contextVectors
    }
}

// MARK: - Gemma3nAudioCumulativeGroupNorm

class Gemma3nAudioCumulativeGroupNorm: Module {

    init(_ config: Gemma3NConfiguration) {
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let expectedInputSuffix = featureDims + (numChannels,)
        // if hiddenStates.shape[2:] != expectedInputSuffix { ... }
        let inputDtype = hiddenStates.dtype
        let calcDtype = torch.float32
        let xCalc = hiddenStates
        let maskCalc = torch.onesLike(xCalc, dtype=calcDtype)
        let sumValuesAtT = torch.sum(xCalc, dim=reductionAxes, keepdim=true)
        let cumSumValues = torch.cumsum(sumValuesAtT, dim=1)
        let elementsInGroupAtT = torch.sum(maskCalc, dim=reductionAxes, keepdim=true)
        let cumCountElements = torch.cumsum(elementsInGroupAtT, dim=1)
        let safeCumCountElements = torch.clamp(cumCountElements, min=1.0)
        let cumMean = cumSumValues / safeCumCountElements
        let squaredDiffFromMean = (xCalc - cumMean).pow(2)
        let sumSqDiffAtT = torch.sum(squaredDiffFromMean, dim=reductionAxes, keepdim=true)
        let cumSumSqDiff = torch.cumsum(sumSqDiffAtT, dim=1)
        let cumVariance = cumSumSqDiff / safeCumCountElements
        let normalizedX = (xCalc - cumMean) * rsqrt(cumVariance + eps)
        let scale = weight
        let scaleViewShape = [1] * (hiddenStates.dim() - 1) + [numChannels]
        let normalizedX = normalizedX * scale.reshaped(scaleViewShape)
        let finalOutput = normalizedX * maskCalc
        return finalOutput
    }
}

// MARK: - Gemma3nAudioSSCPConvBlock

class Gemma3nAudioSSCPConvBlock: Module {

    init(_ config: Gemma3NConfiguration) {
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray) -> MLXArray {
        let audioEncodingsPadded = F.pad(audioEncodings, manualPadding, mode='constant', value=0.0)
        let audioEncodingsConv = conv(audioEncodingsPadded)
        let xForNorm = audioEncodingsConv.permute(0, 2, 3, 1)
        let xNormed = norm(xForNorm)
        let audioEncodingsNormed = xNormed.permute(0, 3, 1, 2)
        return activation(audioEncodingsNormed)
    }
}

// MARK: - Gemma3nAudioSubSampleConvProjection

class Gemma3nAudioSubSampleConvProjection: Module {
    @ModuleInfo(key: "input_proj_linear") var inputProjLinear: Linear

    init(_ config: Gemma3NConfiguration) {
        self._inputProjLinear.wrappedValue = Linear(inputProjInFeatures, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray) -> MLXArray {
        let audioEncodingsReshaped = audioEncodings.expandedDimensions(axis: 1)
        let x = conv_0(audioEncodingsReshaped)
        let x = conv_1(x)
        let (b, cOut, TOut, FOut) = x.shape
        let xPermuted = x.permute(0, 2, 3, 1)
        let outputFlattened = xPermuted.reshaped(b, tOut, fOut * cOut)
        let output = inputProjLinear(outputFlattened)
        return output
    }
}

// MARK: - Gemma3nAudioConformerAttention

class Gemma3nAudioConformerAttention: Module {
    @ModuleInfo(key: "post") var post: Linear

    init(_ config: Gemma3NConfiguration) {
        self._post.wrappedValue = Linear(postInFeatures, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray, _ audioMelMask: MLXArray) -> MLXArray {
        let audioEncodingsInputToAttn = audioEncodings
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        let audioEncodingsNorm = preAttnNorm(audioEncodings)
        let audioEncodingsAttnOut = attn(audioEncodingsNorm, audioMelMask)
        let (b, t, numHeads, HeadDim) = audioEncodingsAttnOut.shape
        let audioEncodingsReshaped = audioEncodingsAttnOut.reshaped(b, t, numHeads * headDim)
        let audioEncodings = post(audioEncodingsReshaped)
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        return audioEncodingsInputToAttn + postNorm(audioEncodings)
    }
}

// MARK: - Gemma3nAudioConformerFeedForward

class Gemma3nAudioConformerFeedForward: Module {
    @ModuleInfo(key: "ffw_layer_1") var ffwLayer1: Linear
    @ModuleInfo(key: "ffw_layer_2") var ffwLayer2: Linear

    init(_ config: Gemma3NConfiguration) {
        self._ffwLayer1.wrappedValue = Linear(config.hiddenSize, config.hiddenSize * 4, bias: false)
        self._ffwLayer2.wrappedValue = Linear(config.hiddenSize * 4, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray) -> MLXArray {
        let residual = audioEncodings
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        let audioEncodings = preLayerNorm(audioEncodings)
        // audio_encodings: torch.Tensor = self.ffw_layer_1(audio_encod...
        let audioEncodings = nn.functional.silu(audioEncodings)
        // audio_encodings: torch.Tensor = self.ffw_layer_2(audio_encod...
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        let audioEncodings = postLayerNorm(audioEncodings)
        return residual + audioEncodings * postLayerScale
    }
}

// MARK: - Gemma3nAudioConformerLightConv1d

class Gemma3nAudioConformerLightConv1d: Module {
    @ModuleInfo(key: "linear_start") var linearStart: Linear
    @ModuleInfo(key: "linear_end") var linearEnd: Linear

    init(_ config: Gemma3NConfiguration) {
        self._linearStart.wrappedValue = Linear(config.hiddenSize, config.hiddenSize * 2, bias: false)
        self._linearEnd.wrappedValue = Linear(config.hiddenSize, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray) -> MLXArray {
        let audioEncodingsResidual = audioEncodings
        let audioEncodings = preLayerNorm(audioEncodings)
        let audioEncodings = linearStart(audioEncodings)
        let audioEncodings = torch.nn.functional.glu(audioEncodings, dim=-1)
        let audioEncodingsPermuted = audioEncodings.permute(0, 2, 1)
        let audioEncodingsPermutedPadded = F.pad(audioEncodingsPermuted, (causalPadding, 0))
        let audioEncodings = depthwise_conv1d(audioEncodingsPermutedPadded)
        let audioEncodings = audioEncodings.permute(0, 2, 1)
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        let audioEncodings = convNorm(audioEncodings)
        let audioEncodings = nn.functional.silu(audioEncodings)
        let audioEncodings = linearEnd(audioEncodings)
        let output = audioEncodings + audioEncodingsResidual
        return output
    }
}

// MARK: - Gemma3nAudioConformerBlock

class Gemma3nAudioConformerBlock: Module {

    init(_ config: Gemma3NConfiguration) {
        super.init()
    }

    func callAsFunction(_ audioEncodings: MLXArray, _ audioMelMask: MLXArray) -> MLXArray {
        let audioEncodings = ffwLayerStart(audioEncodings)
        let audioEncodings = attention(audioEncodings, audioMelMask)
        let validityMaskForLconv = ~audioMelMask
        let audioEncodingsForLconvInput = audioEncodings * validityMaskForLconv.unsqueeze(-1)
        let audioEncodings = lconv1d(audioEncodingsForLconvInput)
        let audioEncodings = ffwLayerEnd(audioEncodings)
        let audioEncodings = torch.clamp(audioEncodings, -gradientClipping, gradientClipping)
        let output = norm(audioEncodings)
        return output
    }
}

// MARK: - Gemma3nAudioEncoder

class Gemma3nAudioEncoder: Module {
    @ModuleInfo(key: "conformer") var conformer: Array

    init(_ config: Gemma3NConfiguration) {
        self._conformer.wrappedValue = Array([Gemma3nAudioConformerBlock(config) for _ in range(config.confNumHiddenLayers)])
        super.init()
    }

    func callAsFunction(_ audioMel: MLXArray, _ audioMelMask: MLXArray) -> MLXArray {
        let audioEncodings = subsampleConvProjection(audioMel)
        let tSub = audioEncodings.dim(1)
        let timeStrideProduct = 1
        // for loop: for stride_pair_idx in range(len(self.config.sscp_...
        let indices = MLXArray(0..<tSub, device=audioMelMask.device) * timeStrideProduct
        let indices = torch.clamp(indices, max=audioMelMask.dim(1) - 1)
        // if audioMelMask.ndim > 1 and indices.ndim == 1 { ... }
        let currentMask = torch.gather(audioMelMask, 1, indices)
        // for loop: for block in self.conformer:
    audio_encodings =...
        // if config.confReductionFactor > 1 { ... }
        let audioEncodings = audioEncodings.maskedFill(currentMask.unsqueeze(-1), 0.0)
        return (audioEncodings, currentMask)
    }
}

// MARK: - Gemma3nTextLaurelBlock

class Gemma3nTextLaurelBlock: Module {
    @ModuleInfo(key: "linear_left") var linearLeft: Linear
    @ModuleInfo(key: "linear_right") var linearRight: Linear

    init(_ config: Gemma3NConfiguration) {
        self._linearLeft.wrappedValue = Linear(config.hiddenSize, config.laurelRank, bias: false)
        self._linearRight.wrappedValue = Linear(config.laurelRank, config.hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        // laurel_hidden_states: torch.Tensor = self.linear_left(hidden...
        // laurel_hidden_states: torch.Tensor = self.linear_right(laure...
        let normedLaurelHiddenStates = postLaurelNorm(laurelHiddenStates)
        return hiddenStates + normedLaurelHiddenStates
    }
}

// MARK: - Gemma3nTextMLP

class Gemma3nTextMLP: Module {
    @ModuleInfo(key: "gate_proj") var gateProj: Linear
    @ModuleInfo(key: "up_proj") var upProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: Gemma3NConfiguration) {
        self._gateProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)
        self._upProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)
        self._downProj.wrappedValue = Linear(intermediateSize, hiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray) -> MLXArray {
        let gateProj = gateProj(hiddenStates)
        // if activationSparsity > 0.0 { ... }
        let activations = actFn(gateProj)
        let upProj = upProj(hiddenStates)
        let downProj = downProj(activations * upProj)
        return downProj
    }
}

// MARK: - Gemma3nTextAltUp

class Gemma3nTextAltUp: Module {
    @ModuleInfo(key: "correction_coefs") var correctionCoefs: Linear
    @ModuleInfo(key: "prediction_coefs") var predictionCoefs: Linear
    @ModuleInfo(key: "modality_router") var modalityRouter: Linear

    init(_ config: Gemma3NConfiguration) {
        self._correctionCoefs.wrappedValue = Linear(config.altupNumInputs, config.altupNumInputs, bias: false)
        self._predictionCoefs.wrappedValue = Linear(config.altupNumInputs, config.altupNumInputs ** 2, bias: false)
        self._modalityRouter.wrappedValue = Linear(config.hiddenSize, config.altupNumInputs, bias: false)
        super.init()
    }

    func callAsFunction(_ corrected: MLXArray) -> MLXArray {
        return (corrected.typeAs(correctOutputScale) * correctOutputScale).asType(corrected.dtype)
    }
}

// MARK: - Gemma3nTextAttention

class Gemma3nTextAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear

    init(_ config: Gemma3NConfiguration) {
        self._qProj.wrappedValue = Linear(config.hiddenSize, config.numAttentionHeads * headDim, bias: config.attentionBias)
        self._kProj.wrappedValue = Linear(config.hiddenSize, config.numKeyValueHeads * headDim, bias: config.attentionBias)
        self._vProj.wrappedValue = Linear(config.hiddenSize, config.numKeyValueHeads * headDim, bias: config.attentionBias)
        self._oProj.wrappedValue = Linear(config.numAttentionHeads * headDim, config.hiddenSize, bias: config.attentionBias)
        super.init()
    }

    func callAsFunction(_ hiddenStates: MLXArray, _ positionEmbeddings: MLXArray, _ attentionMask: MLXArray, _ pastKeyValues: MLXArray, _ cachePosition: MLXArray) -> MLXArray {
        let inputShape = hiddenStates.shape[:-1]
        let hiddenShape = (*inputShape, -1, config.headDim)
        let (cos, sin) = positionEmbeddings
        let queryStates = qProj(hiddenStates).reshaped(hiddenShape)
        let queryStates = qNorm(queryStates)
        let queryStates = applyRotaryPosEmb(queryStates, cos, sin, unsqueezeDim=2)
        let queryStates = queryStates.transposed(1, 2)
        // if isKvSharedLayer and pastKeyValues is not nil { ... }
        // if pastKeyValues is not nil { ... }
        // attention_interface: Callable = eager_attention_forward...
        // if config._attn_implementation != 'eager' { ... }
        let (attnOutput, AttnWeights) = attentionInterface(self, queryStates, keyStates, valueStates, attentionMask, dropout=attentionDropout if training else 0.0, scaling=scaling, slidingWindow=slidingWindow, **kwargs)
        let attnOutput = attnOutput.reshaped(*inputShape, -1)
        let attnOutput = oProj(attnOutput)
        return (attnOutput, attnWeights)
    }
}

// MARK: - Gemma3nRotaryEmbedding

class Gemma3nRotaryEmbedding: Module {

    init(_ config: Gemma3NConfiguration) {
        super.init()
    }

    func callAsFunction(_ x: MLXArray, _ positionIds: MLXArray, _ layerType: MLXArray) -> MLXArray {
        let invFreq = getattr(self, f'{layerType}_inv_freq')
        let attentionScaling = getattr(self, f'{layerType}_attention_scaling')
        let invFreqExpanded = invFreq[nil, :, nil].asType(.float32).expand(positionIds.dim(0), -1, 1)
        let positionIdsExpanded = positionIds[:, nil, :].asType(.float32)
        let deviceType = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'
        // with block (skipped)
        return (cos, sin)
    }
}

// MARK: - Gemma3nMultimodalEmbedder

class Gemma3nMultimodalEmbedder: Module {
    @ModuleInfo(key: "embedding") var embedding: Embedding
    @ModuleInfo(key: "embedding_projection") var embeddingProjection: Linear

    init(_ config: Gemma3NConfiguration) {
        self._embedding.wrappedValue = Embedding(vocabSize, multimodalHiddenSize)
        self._embeddingProjection.wrappedValue = Linear(multimodalHiddenSize, textHiddenSize, bias: false)
        super.init()
    }

    func callAsFunction(_ inputIds: MLXArray, _ inputsEmbeds: MLXArray) -> MLXArray {
        // if (inputIds is nil) ^ (inputsEmbeds is not nil) { ... }
        // if inputsEmbeds is not nil { ... }
        let embNormProj = embeddingProjection(embNorm)
        return embeddingPostProjectionNorm(embNormProj)
    }
}
