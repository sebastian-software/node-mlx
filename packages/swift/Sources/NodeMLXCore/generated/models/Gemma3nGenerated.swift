//
//  Gemma3nGenerated.swift
//  NodeMLXCore
//
//  AUTO-GENERATED FILE - DO NOT EDIT MANUALLY!
//  Generated by hf2swift from model patterns.
//  Re-run the generator to update this file.
//
//  Based on patterns from mlx-lm and mlx-swift-lm.
//

import Foundation
import MLX
import MLXFast
import MLXNN

// MARK: - Configuration

public struct Gemma3nConfiguration: Decodable, Sendable {
    public var hiddenSize: Int
    public var numHiddenLayers: Int
    public var numAttentionHeads: Int
    public var numKeyValueHeads: Int
    public var intermediateSizes: [Int] // Per-layer intermediate sizes
    public var vocabSize: Int
    public var headDim: Int
    public var rmsNormEps: Float
    public var ropeTheta: Float
    public var maxPositionEmbeddings: Int
    public var attentionBias: Bool
    public var mlpBias: Bool
    public var slidingWindow: Int
    public var layerTypes: [String]
    public var ropeLocalBaseFreq: Float
    public var numKVSharedLayers: Int

    // AltUp configuration
    public var altupNumInputs: Int
    public var altupActiveIdx: Int
    public var altupCorrectScale: Bool
    public var altupCoefClip: Float?
    public var laurelRank: Int
    public var hiddenSizePerLayerInput: Int
    public var vocabSizePerLayerInput: Int
    public var activationSparsityPattern: [Float]
    public var finalLogitSoftcapping: Float?
    public var ropeScaling: [String: StringOrNumber]?
    public var modelType: String?

    /// Get intermediate size for a specific layer
    public func intermediateSize(forLayer idx: Int) -> Int {
        if idx < intermediateSizes.count {
            return intermediateSizes[idx]
        }
        return intermediateSizes.first ?? 16384
    }

    /// First KV shared layer index
    public var firstKVSharedLayerIdx: Int {
        numHiddenLayers - numKVSharedLayers
    }

    /// Check if a layer uses shared KV cache
    public func isKVSharedLayer(_ layerIdx: Int) -> Bool {
        layerIdx >= firstKVSharedLayerIdx
    }

    /// Check if a layer is a global attention layer
    public func isGlobalLayer(_ layerIdx: Int) -> Bool {
        if layerIdx < layerTypes.count {
            let layerType = layerTypes[layerIdx].lowercased()
            return layerType == "full_attention" || layerType == "global"
        }
        return false
    }

    enum CodingKeys: String, CodingKey {
        case textConfig = "text_config"
        case hiddenSize = "hidden_size"
        case numHiddenLayers = "num_hidden_layers"
        case numAttentionHeads = "num_attention_heads"
        case numKeyValueHeads = "num_key_value_heads"
        case intermediateSize = "intermediate_size"
        case vocabSize = "vocab_size"
        case headDim = "head_dim"
        case rmsNormEps = "rms_norm_eps"
        case ropeTheta = "rope_theta"
        case maxPositionEmbeddings = "max_position_embeddings"
        case attentionBias = "attention_bias"
        case mlpBias = "mlp_bias"
        case slidingWindow = "sliding_window"
        case layerTypes = "layer_types"
        case ropeLocalBaseFreq = "rope_local_base_freq"
        case numKVSharedLayers = "num_kv_shared_layers"
        case altupNumInputs = "altup_num_inputs"
        case altupActiveIdx = "altup_active_idx"
        case altupCorrectScale = "altup_correct_scale"
        case altupCoefClip = "altup_coef_clip"
        case laurelRank = "laurel_rank"
        case hiddenSizePerLayerInput = "hidden_size_per_layer_input"
        case vocabSizePerLayerInput = "vocab_size_per_layer_input"
        case activationSparsityPattern = "activation_sparsity_pattern"
        case finalLogitSoftcapping = "final_logit_softcapping"
        case ropeScaling = "rope_scaling"
        case modelType = "model_type"
    }

    public init(from decoder: Swift.Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)

        // Helper to decode from text_config or top level
        func decode<T: Decodable>(_ key: CodingKeys, default defaultValue: T? = nil) throws -> T {
            if let nested = try? container.nestedContainer(keyedBy: CodingKeys.self, forKey: .textConfig),
               let value = try? nested.decode(T.self, forKey: key)
            {
                return value
            }
            if let value = try? container.decode(T.self, forKey: key) {
                return value
            }
            if let defaultValue {
                return defaultValue
            }
            throw DecodingError.keyNotFound(key, DecodingError.Context(codingPath: [], debugDescription: "Missing \(key)"))
        }

        hiddenSize = try decode(.hiddenSize)
        numHiddenLayers = try decode(.numHiddenLayers)
        numAttentionHeads = try decode(.numAttentionHeads)
        numKeyValueHeads = try decode(.numKeyValueHeads, default: numAttentionHeads)

        // intermediate_size can be a single Int or [Int] array
        if let sizes: [Int] = try? decode(.intermediateSize) {
            intermediateSizes = sizes
        } else if let size: Int = try? decode(.intermediateSize) {
            intermediateSizes = Array(repeating: size, count: numHiddenLayers)
        } else {
            intermediateSizes = Array(repeating: 16384, count: numHiddenLayers)
        }

        vocabSize = try decode(.vocabSize)
        headDim = try decode(.headDim, default: hiddenSize / numAttentionHeads)
        rmsNormEps = try decode(.rmsNormEps, default: 0.000001)
        ropeTheta = try decode(.ropeTheta, default: 1_000_000.0)
        maxPositionEmbeddings = try decode(.maxPositionEmbeddings, default: 32768)
        attentionBias = try decode(.attentionBias, default: false)
        mlpBias = try decode(.mlpBias, default: false)

        slidingWindow = try decode(.slidingWindow, default: 512)

        if let types: [String] = try? decode(.layerTypes) {
            layerTypes = types
        } else {
            layerTypes = []
        }

        ropeLocalBaseFreq = try decode(.ropeLocalBaseFreq, default: 10000.0)
        numKVSharedLayers = try decode(.numKVSharedLayers, default: 0)

        // AltUp configuration with defaults
        altupNumInputs = try decode(.altupNumInputs, default: 4)
        altupActiveIdx = try decode(.altupActiveIdx, default: 0)
        altupCorrectScale = try decode(.altupCorrectScale, default: true)
        altupCoefClip = try? decode(.altupCoefClip) as Float

        laurelRank = try decode(.laurelRank, default: 64)
        hiddenSizePerLayerInput = try decode(.hiddenSizePerLayerInput, default: 256)
        vocabSizePerLayerInput = try decode(.vocabSizePerLayerInput, default: 262_144)

        if let pattern: [Float] = try? decode(.activationSparsityPattern) {
            activationSparsityPattern = pattern
        } else {
            activationSparsityPattern = []
        }

        finalLogitSoftcapping = try? decode(.finalLogitSoftcapping)
        ropeScaling = try? container.decode([String: StringOrNumber].self, forKey: .ropeScaling)
        modelType = try? container.decode(String.self, forKey: .modelType)
    }
}

// MARK: - RMS Norm Variants

/// RMSNorm without learnable scale weights - used for value normalization
class RMSNoScale: Module {
    let eps: Float

    init(eps: Float = 1e-5) {
        self.eps = eps
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let variance = mean(x.pow(2), axis: -1, keepDims: true)
        return x * rsqrt(variance + eps)
    }
}

/// Uses shared RMSNorm implementation
typealias Gemma3nRMSNorm = RMSNorm

// MARK: - Utility Functions

// MARK: - AltUp Block

/// Alternating Updates module for efficient sparse computation
class Gemma3nAltUp: Module {
    let numInputs: Int
    let activeIdx: Int
    let hiddenSize: Int
    let altupCoefClip: Float?

    @ModuleInfo(key: "correct_output_scale") var correctOutputScale: MLXArray
    @ModuleInfo(key: "correction_coefs") var correctionCoefs: Linear
    @ModuleInfo(key: "prediction_coefs") var predictionCoefs: Linear
    @ModuleInfo(key: "modality_router") var modalityRouter: Linear
    @ModuleInfo(key: "router_norm") var routerNorm: Gemma3nRMSNorm

    init(_ config: Gemma3nConfiguration) {
        numInputs = config.altupNumInputs
        activeIdx = config.altupActiveIdx
        hiddenSize = config.hiddenSize
        altupCoefClip = config.altupCoefClip

        _correctOutputScale.wrappedValue = MLXArray.zeros([config.hiddenSize])
        _correctionCoefs.wrappedValue = Linear(numInputs, numInputs, bias: false)
        _predictionCoefs.wrappedValue = Linear(numInputs, numInputs * numInputs, bias: false)
        _modalityRouter.wrappedValue = Linear(config.hiddenSize, numInputs, bias: false)
        _routerNorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    func computeRouterModalities(_ x: MLXArray) -> MLXArray {
        let routerInputs = routerNorm(x) * pow(Float(hiddenSize), -1.0)
        let routed = modalityRouter(routerInputs).asType(.float32)
        return tanh(routed)
    }

    /// Predict step: modifies input using learned coefficients
    /// Input: [numInputs, batch, seq, hidden] -> Output: [numInputs, batch, seq, hidden]
    func predict(_ hiddenStates: MLXArray) -> MLXArray {
        let modalities = computeRouterModalities(hiddenStates[activeIdx])

        // Compute prediction coefficients with optional clipping
        var weight = predictionCoefs.weight.asType(.float32)
        if let clipVal = altupCoefClip {
            weight = clip(weight, min: -clipVal, max: clipVal)
        }

        // Manual linear: modalities @ weight.T
        var allCoefs = matmul(modalities.asType(.float32), weight.T)
        let shape = modalities.shape
        allCoefs = allCoefs.reshaped([shape[0], shape[1], numInputs, numInputs])
        allCoefs = allCoefs.transposed(0, 1, 3, 2)

        // Convert to float32 for better precision
        let xUp = hiddenStates.asType(.float32)
        let xPermuted = xUp.transposed(1, 2, 3, 0)
        var predictions = matmul(xPermuted, allCoefs)
        predictions = predictions.transposed(3, 0, 1, 2)
        predictions = predictions + xUp

        return predictions.asType(hiddenStates.dtype)
    }

    /// Correct step: refines predictions based on activated output
    func correct(_ predictions: MLXArray, activated: MLXArray) -> MLXArray {
        let modalities = computeRouterModalities(activated)

        // Compute correction coefficients with optional clipping
        var weight = correctionCoefs.weight.asType(.float32)
        if let clipVal = altupCoefClip {
            weight = clip(weight, min: -clipVal, max: clipVal)
        }

        // Manual linear + 1.0: modalities @ weight.T + 1.0
        var allCoefs = matmul(modalities.asType(.float32), weight.T) + 1.0
        let activeX = predictions[activeIdx]
        let innovation = activated - activeX

        // allCoefs: [batch, seq, numInputs] -> [numInputs, batch, seq]
        allCoefs = allCoefs.transposed(2, 0, 1)

        // innovation: [batch, seq, hidden]
        // We need to broadcast: [numInputs, batch, seq, 1] * [1, batch, seq, hidden]
        let innovationExpanded = innovation.expandedDimensions(axis: 0)
        let allCoefsExpanded = allCoefs.expandedDimensions(axis: -1)
        let corrected = innovationExpanded * allCoefsExpanded + predictions

        return corrected.asType(activated.dtype)
    }

    func scaleCorrectOutput(_ corrected: MLXArray) -> MLXArray {
        corrected * correctOutputScale
    }
}

// MARK: - Laurel Block

/// Low-rank residual layer (Learned Augmented Residual)
/// Note: This layer adds the residual internally (returns x + laurel_output)
class Gemma3nLaurelBlock: Module {
    @ModuleInfo(key: "linear_left") var linearLeft: Linear
    @ModuleInfo(key: "linear_right") var linearRight: Linear
    @ModuleInfo(key: "post_laurel_norm") var postLaurelNorm: Gemma3nRMSNorm

    init(_ config: Gemma3nConfiguration) {
        _linearLeft.wrappedValue = Linear(config.hiddenSize, config.laurelRank, bias: false)
        _linearRight.wrappedValue = Linear(config.laurelRank, config.hiddenSize, bias: false)
        _postLaurelNorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        var laurel = linearLeft(x)
        laurel = linearRight(laurel)
        laurel = postLaurelNorm(laurel)
        // Add residual connection
        return x + laurel
    }
}

// MARK: - Attention

class Gemma3nAttention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear
    @ModuleInfo(key: "q_norm") var qNorm: Gemma3nRMSNorm
    @ModuleInfo(key: "k_norm") var kNorm: Gemma3nRMSNorm
    @ModuleInfo(key: "v_norm") var vNorm: RMSNoScale

    let numHeads: Int
    let numKVHeads: Int
    let headDim: Int
    let scale: Float
    let rope: RoPE
    let isSliding: Bool
    let isKVSharedLayer: Bool

    init(_ config: Gemma3nConfiguration, layerIdx: Int) {
        numHeads = config.numAttentionHeads
        numKVHeads = config.numKeyValueHeads
        headDim = config.headDim
        scale = 1

        let qDim = numHeads * headDim
        let kvDim = numKVHeads * headDim
        let attnBias = config.attentionBias

        _qProj.wrappedValue = Linear(config.hiddenSize, qDim, bias: attnBias)
        _kProj.wrappedValue = Linear(config.hiddenSize, kvDim, bias: attnBias)
        _vProj.wrappedValue = Linear(config.hiddenSize, kvDim, bias: attnBias)
        _oProj.wrappedValue = Linear(qDim, config.hiddenSize, bias: attnBias)
        _qNorm.wrappedValue = Gemma3nRMSNorm(dimensions: headDim, eps: config.rmsNormEps)
        _kNorm.wrappedValue = Gemma3nRMSNorm(dimensions: headDim, eps: config.rmsNormEps)
        _vNorm.wrappedValue = RMSNoScale(eps: config.rmsNormEps)
        isSliding = !config.isGlobalLayer(layerIdx)
        let ropeBase = isSliding ? config.ropeLocalBaseFreq : config.ropeTheta
        rope = RoPE(dimensions: headDim, traditional: false, base: ropeBase)
        isKVSharedLayer = config.isKVSharedLayer(layerIdx)
    }

    func callAsFunction(
        _ hiddenStates: MLXArray,
        mask: MLXFast.ScaledDotProductAttentionMaskMode,
        cache: inout KVCache?
    ) -> MLXArray {
        let (B, L, _) = (hiddenStates.dim(0), hiddenStates.dim(1), hiddenStates.dim(2))

        var queries = qProj(hiddenStates).reshaped([B, L, numHeads, headDim])
        queries = qNorm(queries)
        queries = queries.transposed(0, 2, 1, 3)

        var keys: MLXArray
        var values: MLXArray
        var offset: Int

        if isKVSharedLayer, let c = cache, let state = c.state {
            // For KV-shared layers, retrieve KV from the designated cache
            keys = state.keys
            values = state.values
            offset = c.offset
        } else {
            // Compute KV for this layer
            offset = cache?.offset ?? 0
            keys = kProj(hiddenStates).reshaped([B, L, numKVHeads, headDim])
            keys = kNorm(keys)
            keys = keys.transposed(0, 2, 1, 3)
            keys = rope(keys, offset: offset)
            values = vProj(hiddenStates).reshaped([B, L, numKVHeads, headDim])
            values = vNorm(values)
            values = values.transposed(0, 2, 1, 3)
            if let c = cache {
                (keys, values) = c.update(keys: keys, values: values)
            }
        }
        queries = rope(queries, offset: offset)

        // Attention using MLXFast (handles GQA automatically)
        let output = MLXFast.scaledDotProductAttention(
            queries: queries,
            keys: keys,
            values: values,
            scale: scale,
            mask: mask
        )

        // Reshape back: [B, heads, L, headDim] -> [B, L, hidden]
        let outputReshaped = output.transposed(0, 2, 1, 3).reshaped([B, L, -1])
        return oProj(outputReshaped)
    }
}

// MARK: - MLP

class Gemma3nMLP: Module {
    @ModuleInfo(key: "gate_proj") var gateProj: Linear
    @ModuleInfo(key: "up_proj") var upProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    let activationSparsity: Float
    let stdMultiplier: Float?

    init(_ config: Gemma3nConfiguration, layerIdx: Int = 0) {
        let intermediateSize = config.intermediateSize(forLayer: layerIdx)
        _gateProj.wrappedValue = Linear(config.hiddenSize, intermediateSize, bias: false)
        _upProj.wrappedValue = Linear(config.hiddenSize, intermediateSize, bias: false)
        _downProj.wrappedValue = Linear(intermediateSize, config.hiddenSize, bias: false)

        // Get activation sparsity for this layer
        if layerIdx < config.activationSparsityPattern.count {
            activationSparsity = config.activationSparsityPattern[layerIdx]
        } else {
            activationSparsity = 0.0
        }

        // Precompute std multiplier for gelu_topk if sparsity > 0
        if activationSparsity > 0 {
            // sqrt(2) * erfinv(2 * sparsity - 1)
            stdMultiplier = Float(sqrt(2.0)) * Self.erfinv(2.0 * activationSparsity - 1.0)
        } else {
            stdMultiplier = nil
        }
    }

    /// Approximate inverse error function
    private static func erfinv(_ x: Float) -> Float {
        let a: Float = 0.147
        let sign: Float = x < 0 ? -1 : 1
        let x2 = x * x
        let lnTerm = log(1 - x2)
        let term1 = 2 / (Float.pi * a) + lnTerm / 2
        let term2 = lnTerm / a
        return sign * sqrt(sqrt(term1 * term1 - term2) - term1)
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        let gateOutput = gateProj(x)
        let activations: MLXArray

        if let stdMult = stdMultiplier, activationSparsity > 0 {
            // gelu_topk: sparse activation
            let inputMean = mean(gateOutput, axis: -1, keepDims: true)
            let inputStd = sqrt(mean((gateOutput - inputMean).pow(2), axis: -1, keepDims: true))
            let cutoffX = inputMean + inputStd * stdMult
            activations = geluApproximate(maximum(MLXArray(Float(0)), gateOutput - cutoffX))
        } else {
            activations = geluApproximate(gateOutput)
        }

        return downProj(activations * upProj(x))
    }
}

// MARK: - Decoder Layer (with AltUp)

class Gemma3nDecoderLayer: Module {
    let layerIdx: Int
    let activeIdx: Int
    let altupCorrectScale: Bool

    @ModuleInfo(key: "self_attn") var selfAttn: Gemma3nAttention
    @ModuleInfo(key: "mlp") var mlp: Gemma3nMLP
    @ModuleInfo(key: "input_layernorm") var inputLayernorm: Gemma3nRMSNorm
    @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayernorm: Gemma3nRMSNorm
    @ModuleInfo(key: "pre_feedforward_layernorm") var preFeedforwardLayernorm: Gemma3nRMSNorm
    @ModuleInfo(key: "post_feedforward_layernorm") var postFeedforwardLayernorm: Gemma3nRMSNorm
    @ModuleInfo(key: "altup") var altup: Gemma3nAltUp
    @ModuleInfo(key: "laurel") var laurel: Gemma3nLaurelBlock
    @ModuleInfo(key: "per_layer_input_gate") var perLayerInputGate: Linear
    @ModuleInfo(key: "per_layer_projection") var perLayerProjection: Linear
    @ModuleInfo(key: "post_per_layer_input_norm") var postPerLayerInputNorm: Gemma3nRMSNorm

    init(_ config: Gemma3nConfiguration, layerIdx: Int) {
        self.layerIdx = layerIdx
        activeIdx = config.altupActiveIdx
        altupCorrectScale = config.altupCorrectScale

        _selfAttn.wrappedValue = Gemma3nAttention(config, layerIdx: layerIdx)
        _mlp.wrappedValue = Gemma3nMLP(config, layerIdx: layerIdx)
        _inputLayernorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        _postAttentionLayernorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        _preFeedforwardLayernorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        _postFeedforwardLayernorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        _altup.wrappedValue = Gemma3nAltUp(config)
        _laurel.wrappedValue = Gemma3nLaurelBlock(config)
        _perLayerInputGate.wrappedValue = Linear(config.hiddenSize, config.hiddenSizePerLayerInput, bias: false)
        _perLayerProjection.wrappedValue = Linear(config.hiddenSizePerLayerInput, config.hiddenSize, bias: false)
        _postPerLayerInputNorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    /// Forward pass with AltUp predict/correct
    /// - Parameters:
    ///   - hiddenStates: [numInputs, batch, seq, hidden]
    ///   - perLayerInput: [batch, seq, hiddenPerLayerInput] (if hasPerLayerInputs)
    ///   - mask: attention mask
    ///   - cache: KV cache (for KV-shared layers, the cache contains pre-computed KV)
    /// - Returns: [numInputs, batch, seq, hidden]
    func callAsFunction(
        _ hiddenStates: MLXArray, perLayerInput: MLXArray,
        mask: MLXFast.ScaledDotProductAttentionMaskMode,
        cache: inout KVCache?
    ) -> MLXArray {
        // 1. AltUp predict
        let predictions = altup.predict(hiddenStates)
        let activePrediction = predictions[activeIdx]

        // 2. Input layernorm
        let activePredictionNormed = inputLayernorm(activePrediction)

        // 3. Laurel (adds residual internally)
        let laurelOutput = laurel(activePredictionNormed)

        // 4. Self attention
        var attn = selfAttn(activePredictionNormed, mask: mask, cache: &cache)
        attn = postAttentionLayernorm(attn)

        // 5. Residual + scale with sqrt(2)
        let attnGated = activePrediction + attn
        let sqrtTwoInv = Float(pow(2.0, -0.5))
        let attnLaurel = (attnGated + laurelOutput) * sqrtTwoInv

        // 6. MLP
        let attnNorm = preFeedforwardLayernorm(attnLaurel)
        let attnFfw = mlp(attnNorm)
        let attnFfwNorm = postFeedforwardLayernorm(attnFfw)
        let attnFfwLaurelGated = attnLaurel + attnFfwNorm

        // 7. AltUp correct
        let correctedPredictions = altup.correct(predictions, activated: attnFfwLaurelGated)

        // 8. Scale corrected output if configured
        var firstPrediction = correctedPredictions[activeIdx]
        if altupCorrectScale {
            firstPrediction = altup.scaleCorrectOutput(firstPrediction)
        }

        // 9. Per-layer input gate and projection
        var perLayerOut = perLayerInputGate(firstPrediction)
        perLayerOut = geluApproximate(perLayerOut)
        perLayerOut = perLayerOut * perLayerInput
        perLayerOut = perLayerProjection(perLayerOut)
        perLayerOut = postPerLayerInputNorm(perLayerOut)

        // Update all slots
        var updatedSlots: [MLXArray] = [correctedPredictions[0]]
        for i in 1 ..< correctedPredictions.dim(0) {
            updatedSlots.append(correctedPredictions[i] + perLayerOut)
        }
        return stacked(updatedSlots, axis: 0)
    }
}

// MARK: - Language Model (AltUp)

class Gemma3nLanguageModel: Module {
    let config: Gemma3nConfiguration
    let hiddenSize: Int
    let hiddenSizePerLayerInput: Int
    let vocabSize: Int
    let vocabSizePerLayerInput: Int
    let numHiddenLayers: Int
    let altupNumInputs: Int
    let finalLogitSoftcapping: Float?

    let firstKVSharedLayerIdx: Int
    let layerIdxToCacheIdx: [Int]
    let firstSlidingIdx: Int
    let firstFullIdx: Int

    @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
    @ModuleInfo(key: "embed_tokens_per_layer") var embedTokensPerLayer: Embedding
    @ModuleInfo(key: "per_layer_model_projection") var perLayerModelProjection: Linear
    @ModuleInfo(key: "per_layer_projection_norm") var perLayerProjectionNorm: Gemma3nRMSNorm
    @ModuleInfo(key: "altup_projections") var altupProjections: [Linear]
    @ModuleInfo(key: "altup_unembed_projections") var altupUnembedProjections: [Linear]
    @ModuleInfo(key: "layers") var layers: [Gemma3nDecoderLayer]
    @ModuleInfo(key: "norm") var norm: Gemma3nRMSNorm

    init(_ config: Gemma3nConfiguration) {
        self.config = config
        hiddenSize = config.hiddenSize
        hiddenSizePerLayerInput = config.hiddenSizePerLayerInput
        vocabSize = config.vocabSize
        vocabSizePerLayerInput = config.vocabSizePerLayerInput
        numHiddenLayers = config.numHiddenLayers
        altupNumInputs = config.altupNumInputs
        finalLogitSoftcapping = config.finalLogitSoftcapping

        firstKVSharedLayerIdx = config.firstKVSharedLayerIdx

        var firstSliding = 0
        var firstFull = 0
        for (i, layerType) in config.layerTypes.enumerated() {
            if layerType == "sliding_attention", firstSliding == 0 { firstSliding = i; break }
        }
        for (i, layerType) in config.layerTypes.enumerated() {
            if layerType == "full_attention" { firstFull = i; break }
        }
        firstSlidingIdx = firstSliding
        firstFullIdx = firstFull

        let concreteLayers = Array(config.layerTypes.prefix(config.firstKVSharedLayerIdx))
        var sharedFullIdx = 0
        var sharedSlidingIdx = 0
        for (i, layerType) in concreteLayers.enumerated().reversed() {
            if layerType == "full_attention", sharedFullIdx == 0 { sharedFullIdx = i }
            if layerType == "sliding_attention", sharedSlidingIdx == 0 { sharedSlidingIdx = i }
            if sharedFullIdx > 0, sharedSlidingIdx > 0 { break }
        }

        var mapping: [Int] = []
        for i in 0 ..< config.numHiddenLayers {
            if i < config.firstKVSharedLayerIdx {
                mapping.append(i)
            } else {
                let layerType = i < config.layerTypes.count ? config.layerTypes[i] : "sliding_attention"
                mapping.append(layerType == "full_attention" ? sharedFullIdx : sharedSlidingIdx)
            }
        }
        layerIdxToCacheIdx = mapping

        _embedTokens.wrappedValue = Embedding(embeddingCount: config.vocabSize, dimensions: config.hiddenSize)
        _embedTokensPerLayer.wrappedValue = Embedding(embeddingCount: config.vocabSizePerLayerInput, dimensions: numHiddenLayers * config.hiddenSizePerLayerInput)
        _perLayerModelProjection.wrappedValue = Linear(config.hiddenSize, numHiddenLayers * config.hiddenSizePerLayerInput, bias: false)
        _perLayerProjectionNorm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSizePerLayerInput, eps: config.rmsNormEps)
        _altupProjections.wrappedValue = (1 ..< config.altupNumInputs).map { _ in Linear(config.hiddenSize, config.hiddenSize, bias: false) }
        _altupUnembedProjections.wrappedValue = (1 ..< config.altupNumInputs).map { _ in Linear(config.hiddenSize, config.hiddenSize, bias: false) }
        _layers.wrappedValue = (0 ..< numHiddenLayers).map { idx in Gemma3nDecoderLayer(config, layerIdx: idx) }
        _norm.wrappedValue = Gemma3nRMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    func getPerLayerInputs(_ inputIds: MLXArray) -> MLXArray {
        let mask = inputIds .< Int32(vocabSizePerLayerInput)
        let tokens = MLX.where(mask, inputIds, MLXArray.zeros(like: inputIds))
        let embeds = embedTokensPerLayer(tokens)
        let scaled = embeds * sqrt(Float(hiddenSizePerLayerInput))
        let shape = inputIds.shape
        return scaled.reshaped([shape[0], shape[1], numHiddenLayers, hiddenSizePerLayerInput])
    }

    func projectPerLayerInputs(_ inputsEmbeds: MLXArray, _ perLayerInputs: MLXArray) -> MLXArray {
        var projection = perLayerModelProjection(inputsEmbeds)
        projection = projection * pow(Float(hiddenSize), -0.5)
        let shape = inputsEmbeds.shape
        projection = projection.reshaped([shape[0], shape[1], numHiddenLayers, hiddenSizePerLayerInput])
        projection = perLayerProjectionNorm(projection)
        let sqrtTwoInv = Float(pow(2.0, -0.5))
        return (projection + perLayerInputs) * sqrtTwoInv
    }

    func callAsFunction(_ inputIds: MLXArray, cache: inout [KVCache?]) -> MLXArray {
        var h = embedTokens(inputIds)
        h = h * sqrt(Float(hiddenSize))

        let perLayerInputsRaw = getPerLayerInputs(inputIds)
        let perLayerInputs = projectPerLayerInputs(h, perLayerInputsRaw)

        let targetMagnitude = pow(mean(h.pow(2), axis: -1, keepDims: true), 0.5)
        var hList: [MLXArray] = [h]
        for proj in altupProjections {
            hList.append(proj(h))
        }
        var hiddenStates = stacked(hList, axis: 0)

        let mags = pow(mean(hiddenStates[1...].pow(2), axis: -1, keepDims: true), 0.5)
        let minVal = MLXArray(Float.leastNormalMagnitude)
        let normalizedSlots = hiddenStates[1...] * (targetMagnitude / maximum(mags, minVal))
        hiddenStates = concatenated([hiddenStates[0 ..< 1], normalizedSlots], axis: 0)

        let h0 = hiddenStates[0]
        let globalCache = firstFullIdx < cache.count ? cache[firstFullIdx] : nil
        let globalOffset = globalCache?.offset ?? 0
        let globalMask = createAttentionMask(n: h0.dim(1), offset: globalOffset, windowSize: nil)
        let slidingCache = firstSlidingIdx < cache.count ? cache[firstSlidingIdx] : nil
        let slidingOffset = slidingCache?.offset ?? 0
        let slidingMask = createAttentionMask(n: h0.dim(1), offset: slidingOffset, windowSize: config.slidingWindow)

        for i in 0 ..< layers.count {
            let isGlobal = config.isGlobalLayer(i)
            let mask = isGlobal ? globalMask : slidingMask
            let perLayerInput = perLayerInputs[0..., 0..., i, 0...]
            let cacheIdx = layerIdxToCacheIdx[i]
            if cacheIdx < cache.count {
                hiddenStates = layers[i](hiddenStates, perLayerInput: perLayerInput, mask: mask, cache: &cache[cacheIdx])
            } else {
                var nilCache: KVCache? = nil
                hiddenStates = layers[i](hiddenStates, perLayerInput: perLayerInput, mask: mask, cache: &nilCache)
            }
        }

        let finalTargetMagnitude = pow(mean(hiddenStates[0].pow(2), axis: -1, keepDims: true), 0.5)
        var unembedded: [MLXArray] = [hiddenStates[0]]
        for i in 0 ..< altupUnembedProjections.count {
            unembedded.append(altupUnembedProjections[i](hiddenStates[i + 1]))
        }
        var finalStates = stacked(unembedded, axis: 0)

        let finalMags = pow(mean(finalStates[1...].pow(2), axis: -1, keepDims: true), 0.5)
        let normalizedFinal = finalStates[1...] * (finalTargetMagnitude / maximum(finalMags, minVal))
        finalStates = concatenated([finalStates[0 ..< 1], normalizedFinal], axis: 0)

        var output = mean(finalStates, axis: 0)
        output = norm(output)

        let logits = embedTokens.asLinear(output)
        if let cap = finalLogitSoftcapping { return cap * tanh(logits / cap) }
        return logits
    }
}

// MARK: - Inner Wrapper

class Gemma3nInner: Module {
    @ModuleInfo(key: "language_model") var languageModel: Gemma3nLanguageModel

    init(_ config: Gemma3nConfiguration) {
        _languageModel.wrappedValue = Gemma3nLanguageModel(config)
    }

    func callAsFunction(_ inputIds: MLXArray, cache: inout [KVCache?]) -> MLXArray {
        languageModel(inputIds, cache: &cache)
    }
}

// MARK: - Top-Level Model

public class Gemma3nModel: Module, LLMModel {
    public let vocabularySize: Int
    public let numLayers: Int
    public let numKVHeads: Int
    public let headDim: Int

    @ModuleInfo(key: "model") var model: Gemma3nInner
    private let config: Gemma3nConfiguration

    public var supportsCache: Bool { true }

    public init(_ config: Gemma3nConfiguration) {
        self.config = config
        vocabularySize = config.vocabSize
        numLayers = config.numHiddenLayers
        numKVHeads = config.numKeyValueHeads
        headDim = config.headDim
        _model.wrappedValue = Gemma3nInner(config)
    }

    public func callAsFunction(_ inputIds: MLXArray) -> MLXArray {
        var cache: [KVCache?] = Array(repeating: nil, count: numLayers)
        return model(inputIds, cache: &cache)
    }

    public func callAsFunction(_ inputIds: MLXArray, cache: inout [KVCache]?) -> MLXArray {
        var layerCaches: [KVCache?] = if let existingCache = cache { existingCache.map { $0 as KVCache? } }
        else { Array(repeating: nil, count: numLayers) }
        let output = model(inputIds, cache: &layerCaches)
        cache = layerCaches.compactMap(\.self)
        return output
    }

    public func newCache() -> [KVCache] {
        let numCaches = config.firstKVSharedLayerIdx
        return (0 ..< numCaches).map { i in
            let layerType = i < config.layerTypes.count ? config.layerTypes[i] : "sliding_attention"
            if layerType == "full_attention" { return KVCacheSimple() }
            else { return RotatingKVCache(maxSize: config.slidingWindow, keep: 0) }
        }
    }

    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
        var result: [String: MLXArray] = [:]
        let skipPatterns = ["rotary_emb", "vision_tower", "audio_tower", "embed_audio", "embed_vision"]
        for (key, value) in weights {
            let shouldSkip = skipPatterns.contains { key.contains($0) }
            if shouldSkip { continue }
            var newKey = key
            // Transform language_model.model.X -> model.language_model.X
            if key.hasPrefix("language_model.model.") {
                let suffix = String(key.dropFirst("language_model.model.".count))
                newKey = "model.language_model." + suffix
            }
            // Transform language_model.X (without .model.) -> model.language_model.X
            else if key.hasPrefix("language_model.") {
                let suffix = String(key.dropFirst("language_model.".count))
                newKey = "model.language_model." + suffix
            }
            result[newKey] = value
        }
        return result
    }
}
