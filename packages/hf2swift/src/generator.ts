/**
 * Swift Code Generator
 *
 * Generates MLX Swift model code from parsed Python modules.
 * Output is formatted with SwiftFormat for consistent style.
 */

import { execSync } from "node:child_process"
import { ParsedModule } from "./types.js"
import { toPascal } from "./naming.js"
import { generateConfigFromJson } from "./config.js"

/**
 * Format Swift code using swiftformat
 */
export function formatSwift(code: string): string {
  try {
    return execSync("swiftformat stdin", {
      input: code,
      encoding: "utf-8",
      maxBuffer: 10 * 1024 * 1024
    })
  } catch {
    // If swiftformat fails or isn't installed, return unformatted code
    console.warn("Warning: swiftformat not available, returning unformatted code")
    return code
  }
}

/**
 * Swift Model Generator
 */
export class SwiftGenerator {
  private modelName: string
  private modelNameLower: string
  private parsedModules: Map<string, ParsedModule> = new Map()
  private configClass: string

  constructor(modelName: string) {
    this.modelName = toPascal(modelName)
    this.modelNameLower = modelName.toLowerCase().replace(/-/g, "").replace(/_/g, "")
    this.configClass = `${this.modelName}Configuration`
  }

  /**
   * Generate complete Swift file
   */
  generate(modules: ParsedModule[], configJson?: Record<string, unknown>): string {
    for (const m of modules) {
      this.parsedModules.set(m.name, m)
    }

    const parts: string[] = [
      this.genHeader(),
      configJson ? generateConfigFromJson(configJson, this.modelName) : "",
      this.genHelpers()
    ]

    const ordered = this.sortByDependency(modules)
    const generated = new Set<string>()

    for (const m of ordered) {
      if (generated.has(m.name)) continue

      const skipPatterns = [
        "ForCausalLM",
        "ForConditionalGeneration",
        "PreTrainedModel",
        "OutputWithPast",
        "GenerationMixin",
        "Audio",
        "Multimodal"
      ]
      if (skipPatterns.some((p) => m.name.includes(p))) continue
      if (m.name === `${this.modelName}Model`) continue

      generated.add(m.name)
      parts.push(`// MARK: - ${m.swiftName}\n${this.genModule(m)}`)
    }

    parts.push(this.genModelWrapper(modules))

    const code = parts.filter(Boolean).join("\n\n")
    return formatSwift(code)
  }

  private genHeader(): string {
    return `//
//  ${this.modelName}.swift
//  Auto-generated by hf2swift
//
//  Uses parsed modules from HuggingFace Transformers source.
//  Based on patterns from mlx-swift-lm (MIT License, ml-explore).
//

import Foundation
import MLX
import MLXFast
import MLXNN`
  }

  private sortByDependency(modules: ParsedModule[]): ParsedModule[] {
    const deps = new Map<string, Set<string>>()

    for (const m of modules) {
      deps.set(m.name, new Set())
      for (const attr of m.attributes) {
        if (attr.moduleType && this.parsedModules.has(attr.moduleType)) {
          deps.get(m.name)!.add(attr.moduleType)
        }
      }
    }

    const ordered: ParsedModule[] = []
    const visited = new Set<string>()

    const visit = (name: string): void => {
      if (visited.has(name)) return
      visited.add(name)
      for (const dep of deps.get(name) || []) visit(dep)
      const module = this.parsedModules.get(name)
      if (module) ordered.push(module)
    }

    for (const m of modules) visit(m.name)
    return ordered
  }

  private genHelpers(): string {
    const lower = this.modelNameLower
    return `// MARK: - Helper Functions

private func ${lower}ApplyRotaryPosEmb(_ q: MLXArray, _ k: MLXArray, cos: MLXArray, sin: MLXArray) -> (MLXArray, MLXArray) {
let qEmbed = (q * cos) + (${lower}RotateHalf(q) * sin)
let kEmbed = (k * cos) + (${lower}RotateHalf(k) * sin)
return (qEmbed, kEmbed)
}

private func ${lower}RotateHalf(_ x: MLXArray) -> MLXArray {
let halfDim = x.dim(-1) / 2
let x1 = x[.ellipsis, ..<halfDim]
let x2 = x[.ellipsis, halfDim...]
return concatenated([-x2, x1], axis: -1)
}`
  }

  private genModule(m: ParsedModule): string {
    if (m.name.includes("RMSNorm")) return this.genRmsNorm(m)
    if (m.name.includes("Embedding") && m.baseClasses.includes("nn.Embedding"))
      return this.genScaledEmbedding(m)
    if (m.name.includes("Attention")) return this.genAttention(m)
    if (m.name.includes("MLP")) return this.genMlp(m)
    if (m.name.includes("DecoderLayer")) return this.genDecoderLayer(m)
    if (m.name.includes("RotaryEmbedding")) return this.genRotaryEmbedding(m)
    return this.genGenericModule(m)
  }

  private genRmsNorm(m: ParsedModule): string {
    return `class ${m.swiftName}: Module {
let eps: Float
let weight: MLXArray
let withScale: Bool

init(dimensions: Int, eps: Float = 1e-6, withScale: Bool = true) {
self.eps = eps
self.withScale = withScale
self.weight = withScale ? MLXArray.ones([dimensions]) : MLXArray.ones([dimensions])
}

func callAsFunction(_ x: MLXArray) -> MLXArray {
let variance = x.pow(2).mean(axis: -1, keepDims: true)
let normalized = x * rsqrt(variance + eps)
return withScale ? normalized * weight : normalized
}
}`
  }

  private genScaledEmbedding(m: ParsedModule): string {
    return `class ${m.swiftName}: Embedding {
let embedScale: Float

init(embeddingCount: Int, dimensions: Int, embedScale: Float = 1.0) {
self.embedScale = embedScale
super.init(embeddingCount: embeddingCount, dimensions: dimensions)
}

override func callAsFunction(_ x: MLXArray) -> MLXArray {
return super.callAsFunction(x) * embedScale
}
}`
  }

  private genAttention(m: ParsedModule): string {
    const hasAttr = (name: string) => m.attributes.some((a) => a.swiftName === name)
    const rmsNormType = `${this.modelName}RMSNorm`

    const attrs = m.attributes
      .map((attr) =>
        attr.moduleType === "Array"
          ? `var ${attr.swiftName}: [Module] = []`
          : `@ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`
      )
      .join("\n")

    const projInits = [
      hasAttr("qProj") && `self._qProj.wrappedValue = Linear(hiddenSize, qDim, bias: false)`,
      hasAttr("kProj") && `self._kProj.wrappedValue = Linear(hiddenSize, kvDim, bias: false)`,
      hasAttr("vProj") && `self._vProj.wrappedValue = Linear(hiddenSize, kvDim, bias: false)`,
      hasAttr("oProj") && `self._oProj.wrappedValue = Linear(qDim, hiddenSize, bias: false)`,
      hasAttr("qNorm") &&
        `self._qNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6)`,
      hasAttr("kNorm") &&
        `self._kNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6)`,
      hasAttr("vNorm") &&
        `self._vNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6, withScale: false)`
    ]
      .filter(Boolean)
      .join("\n")

    const normApply = [
      hasAttr("qNorm") && `queries = qNorm(queries)`,
      hasAttr("kNorm") && `keys = kNorm(keys)`,
      hasAttr("vNorm") && `values = vNorm(values)`
    ]
      .filter(Boolean)
      .join("\n")

    return `class ${m.swiftName}: Module {
${attrs}

let numHeads: Int
let numKVHeads: Int
let headDim: Int
let scale: Float

init(_ config: ${this.configClass}) {
self.numHeads = config.numAttentionHeads
self.numKVHeads = config.numKeyValueHeads ?? config.numAttentionHeads
self.headDim = config.headDim ?? (config.hiddenSize / config.numAttentionHeads)
if let scalar = config.queryPreAttnScalar {
self.scale = 1.0 / sqrt(Float(scalar))
} else {
self.scale = 1.0 / sqrt(Float(headDim))
}

let hiddenSize = config.hiddenSize
let kvDim = numKVHeads * headDim
let qDim = numHeads * headDim
${projInits}
}

func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil, cache: KVCache? = nil) -> MLXArray {
let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))

var queries = qProj(x).reshaped([B, L, numHeads, headDim])
var keys = kProj(x).reshaped([B, L, numKVHeads, headDim])
var values = vProj(x).reshaped([B, L, numKVHeads, headDim])
${normApply}

queries = queries.transposed(0, 2, 1, 3)
keys = keys.transposed(0, 2, 1, 3)
values = values.transposed(0, 2, 1, 3)

// KV cache update
if var cache = cache {
(keys, values) = cache.update(keys: keys, values: values)
}

if numKVHeads < numHeads {
let repeats = numHeads / numKVHeads
keys = MLXArray.repeated(keys, count: repeats, axis: 1)
values = MLXArray.repeated(values, count: repeats, axis: 1)
}

var scores = matmul(queries, keys.transposed(0, 1, 3, 2)) * scale
if let mask = mask {
scores = scores + mask
}
let weights = softmax(scores, axis: -1)
let output = matmul(weights, values).transposed(0, 2, 1, 3).reshaped([B, L, -1])
return oProj(output)
}
}`
  }

  private genMlp(m: ParsedModule): string {
    const hasAttr = (name: string) => m.attributes.some((a) => a.swiftName === name)

    const attrs = m.attributes
      .map((attr) => `@ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
      .join("\n")

    const inits = [
      hasAttr("gateProj") &&
        `self._gateProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)`,
      hasAttr("upProj") &&
        `self._upProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)`,
      hasAttr("downProj") &&
        `self._downProj.wrappedValue = Linear(intermediateSize, hiddenSize, bias: false)`
    ]
      .filter(Boolean)
      .join("\n")

    const body = hasAttr("gateProj") ? `return downProj(gelu(gateProj(x)) * upProj(x))` : `return x`

    return `class ${m.swiftName}: Module {
${attrs}

init(_ config: ${this.configClass}) {
let hiddenSize = config.hiddenSize
let intermediateSize = config.intermediateSize
${inits}
}

func callAsFunction(_ x: MLXArray) -> MLXArray {
${body}
}
}`
  }

  private genDecoderLayer(m: ParsedModule): string {
    const rmsNormType = `${this.modelName}RMSNorm`

    const attrs = m.attributes
      .filter((a) => a.moduleType !== "Array")
      .map((attr) => `@ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
      .join("\n")

    const attentionAttr = m.attributes.find((a) => a.moduleType?.includes("Attention"))
    const mlpAttr = m.attributes.find((a) => a.moduleType?.includes("MLP"))

    const inits: string[] = []
    if (attentionAttr)
      inits.push(
        `self._${attentionAttr.swiftName}.wrappedValue = ${attentionAttr.moduleType}(config)`
      )
    if (mlpAttr)
      inits.push(`self._${mlpAttr.swiftName}.wrappedValue = ${mlpAttr.moduleType}(config)`)

    for (const attr of m.attributes) {
      if (attr.moduleType?.includes("Norm")) {
        inits.push(
          `self._${attr.swiftName}.wrappedValue = ${rmsNormType}(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)`
        )
      }
      if (attr.moduleType?.includes("LaurelBlock") || attr.moduleType?.includes("AltUp")) {
        inits.push(`self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      }
    }

    const preAttnNorm = m.attributes.find((a) =>
      ["inputLayernorm", "preAttnNorm", "preFfnNorm"].includes(a.swiftName)
    )
    const postAttnNorm = m.attributes.find((a) =>
      ["postAttentionLayernorm", "postAttnNorm"].includes(a.swiftName)
    )

    const body: string[] = ["var h = x"]
    body.push(preAttnNorm ? `let normed = ${preAttnNorm.swiftName}(h)` : `let normed = h`)
    if (attentionAttr) {
      body.push(`let attnOut = ${attentionAttr.swiftName}(normed, mask: mask, cache: cache)`)
      body.push(`h = h + attnOut`)
    }
    body.push(postAttnNorm ? `let postNormed = ${postAttnNorm.swiftName}(h)` : `let postNormed = h`)
    if (mlpAttr) {
      body.push(`let mlpOut = ${mlpAttr.swiftName}(postNormed)`)
      body.push(`h = h + mlpOut`)
    }
    body.push(`return h`)

    return `class ${m.swiftName}: Module {
${attrs}

init(_ config: ${this.configClass}, layerIdx: Int = 0) {
${inits.join("\n")}
}

func callAsFunction(_ x: MLXArray, mask: MLXArray? = nil, cache: KVCache? = nil) -> MLXArray {
${body.join("\n")}
}
}`
  }

  private genRotaryEmbedding(m: ParsedModule): string {
    return `class ${m.swiftName}: Module {
let dim: Int
let maxPositionEmbeddings: Int
let base: Float

init(_ config: ${this.configClass}) {
self.dim = config.headDim ?? (config.hiddenSize / config.numAttentionHeads)
self.maxPositionEmbeddings = config.maxPositionEmbeddings ?? 8192
self.base = config.ropeTheta ?? 10000.0
}

func callAsFunction(_ x: MLXArray, offset: Int = 0) -> (MLXArray, MLXArray) {
let seqLen = x.dim(1)
let freqs = MLXArray(stride(from: 0, to: Float(dim), by: 2).map { pow(base, -$0 / Float(dim)) })
let positions = MLXArray((offset..<(offset + seqLen)).map { Float($0) })
let angles = positions.expandedDimensions(axis: 1) * freqs.expandedDimensions(axis: 0)
return (cos(angles), sin(angles))
}
}`
  }

  private genGenericModule(m: ParsedModule): string {
    const attrs = m.attributes
      .map((attr) =>
        attr.moduleType === "Array"
          ? `var ${attr.swiftName}: [Module] = []`
          : `@ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`
      )
      .join("\n")

    const inits: string[] = []
    for (const attr of m.attributes) {
      if (attr.moduleType === "Linear") {
        inits.push(
          `self._${attr.swiftName}.wrappedValue = Linear(config.hiddenSize, config.hiddenSize, bias: false)`
        )
      } else if (attr.moduleType === "Embedding") {
        inits.push(
          `self._${attr.swiftName}.wrappedValue = Embedding(embeddingCount: config.vocabSize, dimensions: config.hiddenSize)`
        )
      } else if (attr.moduleType?.includes("RMSNorm")) {
        inits.push(
          `self._${attr.swiftName}.wrappedValue = ${this.modelName}RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)`
        )
      } else if (attr.moduleType?.includes("ScaledWordEmbedding")) {
        inits.push(`self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(
embeddingCount: config.vocabSize,
dimensions: config.hiddenSize,
embedScale: sqrt(Float(config.hiddenSize))
)`)
      } else if (attr.moduleType?.includes("RotaryEmbedding")) {
        inits.push(`self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      } else if (this.parsedModules.has(attr.moduleType || "")) {
        inits.push(`self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      }
    }

    return `class ${m.swiftName}: Module {
${attrs}

init(_ config: ${this.configClass}) {
${inits.join("\n")}
}

func callAsFunction(_ x: MLXArray) -> MLXArray {
return x
}
}`
  }

  private genModelWrapper(modules: ParsedModule[]): string {
    const decoderLayer = modules.find((m) => m.name.includes("DecoderLayer"))
    const decoderLayerName = decoderLayer?.name || "Module"

    return `// MARK: - ${this.modelName}ModelInner

class ${this.modelName}ModelInner: Module {
@ModuleInfo(key: "embed_tokens") var embedTokens: ${this.modelName}TextScaledWordEmbedding
@ModuleInfo(key: "layers") var layers: [${decoderLayerName}]
@ModuleInfo(key: "norm") var norm: ${this.modelName}RMSNorm

init(_ config: ${this.configClass}) {
self._embedTokens.wrappedValue = ${this.modelName}TextScaledWordEmbedding(
embeddingCount: config.vocabSize,
dimensions: config.hiddenSize,
embedScale: config.hiddenSize > 0 ? sqrt(Float(config.hiddenSize)) : 1.0
)
self._layers.wrappedValue = (0..<config.numHiddenLayers).map { ${decoderLayerName}(config, layerIdx: $0) }
self._norm.wrappedValue = ${this.modelName}RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)
}

func callAsFunction(_ inputIds: MLXArray, cache: [KVCache]? = nil) -> MLXArray {
var h = embedTokens(inputIds)
for (i, layer) in layers.enumerated() {
let layerCache = cache?[i]
h = layer(h, mask: nil, cache: layerCache)
}
return norm(h)
}
}

// MARK: - ${this.modelName}Model

public class ${this.modelName}Model: Module, LLMModel {
public let vocabularySize: Int
public let numLayers: Int

@ModuleInfo(key: "model") var model: ${this.modelName}ModelInner
@ModuleInfo(key: "lm_head") var lmHead: Linear
private let config: ${this.configClass}

public init(_ config: ${this.configClass}) {
self.config = config
self.vocabularySize = config.vocabSize
self.numLayers = config.numHiddenLayers
self._model.wrappedValue = ${this.modelName}ModelInner(config)
self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabSize, bias: false)
}

public func callAsFunction(_ inputIds: MLXArray) -> MLXArray {
let h = model(inputIds, cache: nil)
return lmHead(h)
}

public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
var result: [String: MLXArray] = [:]
for (key, value) in weights {
var newKey = key
if newKey.hasPrefix("model.language_model.") {
newKey = String(newKey.dropFirst("model.language_model.".count))
} else if newKey.hasPrefix("language_model.") {
newKey = String(newKey.dropFirst("language_model.".count))
}
result[newKey] = value
}
return result
}
}`
  }
}
