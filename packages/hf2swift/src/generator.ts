/**
 * Swift Code Generator
 *
 * Generates MLX Swift model code from parsed Python modules
 */

import { ParsedModule, ModuleAttribute } from "./types.js"
import { toPascal } from "./naming.js"
import { generateConfigFromJson } from "./config.js"

/**
 * Swift Model Generator
 */
export class SwiftGenerator {
  private modelName: string
  private modelNameLower: string
  private parsedModules: Map<string, ParsedModule> = new Map()
  private configClass: string

  constructor(modelName: string) {
    this.modelName = toPascal(modelName)
    this.modelNameLower = modelName.toLowerCase().replace(/-/g, "").replace(/_/g, "")
    this.configClass = `${this.modelName}Configuration`
  }

  /**
   * Generate complete Swift file
   */
  generate(modules: ParsedModule[], configJson?: Record<string, unknown>): string {
    // Index modules by name
    for (const m of modules) {
      this.parsedModules.set(m.name, m)
    }

    const lines: string[] = [
      "//",
      `//  ${this.modelName}.swift`,
      "//  Auto-generated by hf2swift",
      "//",
      "//  Uses parsed modules from HuggingFace Transformers source.",
      "//  Based on patterns from mlx-swift-lm (MIT License, ml-explore).",
      "//",
      "",
      "import Foundation",
      "import MLX",
      "import MLXFast",
      "import MLXNN",
      ""
    ]

    // Generate config if provided
    if (configJson) {
      lines.push("// MARK: - Configuration")
      lines.push("")
      lines.push(generateConfigFromJson(configJson, this.modelName))
      lines.push("")
    }

    // Generate helpers
    lines.push(...this.genHelpers())
    lines.push("")

    // Sort modules by dependency
    const ordered = this.sortByDependency(modules)

    // Generate modules
    const generated = new Set<string>()
    for (const m of ordered) {
      if (generated.has(m.name)) {
        continue
      }

      // Skip wrapper classes and other non-essential modules
      const skipPatterns = [
        "ForCausalLM",
        "ForConditionalGeneration",
        "PreTrainedModel",
        "OutputWithPast",
        "GenerationMixin",
        "Audio",
        "Multimodal"
      ]
      if (skipPatterns.some((p) => m.name.includes(p))) {
        continue
      }

      // Skip the main model class (we generate our own wrapper)
      if (m.name === `${this.modelName}Model`) {
        continue
      }

      generated.add(m.name)
      lines.push(`// MARK: - ${m.swiftName}`)
      lines.push("")
      lines.push(...this.genModule(m))
      lines.push("")
    }

    // Generate model wrapper
    lines.push(...this.genModelWrapper(modules))

    return lines.join("\n")
  }

  /**
   * Sort modules by dependency order
   */
  private sortByDependency(modules: ParsedModule[]): ParsedModule[] {
    const deps = new Map<string, Set<string>>()

    // Build dependency graph
    for (const m of modules) {
      deps.set(m.name, new Set())
      for (const attr of m.attributes) {
        if (attr.moduleType && this.parsedModules.has(attr.moduleType)) {
          deps.get(m.name)!.add(attr.moduleType)
        }
      }
    }

    // Topological sort
    const ordered: ParsedModule[] = []
    const visited = new Set<string>()

    const visit = (name: string): void => {
      if (visited.has(name)) return
      visited.add(name)

      for (const dep of deps.get(name) || []) {
        visit(dep)
      }

      const module = this.parsedModules.get(name)
      if (module) {
        ordered.push(module)
      }
    }

    for (const m of modules) {
      visit(m.name)
    }

    return ordered
  }

  /**
   * Generate helper functions
   */
  private genHelpers(): string[] {
    return [
      "// MARK: - Helper Functions",
      "",
      `private func ${this.modelNameLower}ApplyRotaryPosEmb(`,
      "    _ q: MLXArray,",
      "    _ k: MLXArray,",
      "    cos: MLXArray,",
      "    sin: MLXArray",
      ") -> (MLXArray, MLXArray) {",
      `    let qEmbed = (q * cos) + (${this.modelNameLower}RotateHalf(q) * sin)`,
      `    let kEmbed = (k * cos) + (${this.modelNameLower}RotateHalf(k) * sin)`,
      "    return (qEmbed, kEmbed)",
      "}",
      "",
      `private func ${this.modelNameLower}RotateHalf(_ x: MLXArray) -> MLXArray {`,
      "    let halfDim = x.dim(-1) / 2",
      "    let x1 = x[.ellipsis, ..<halfDim]",
      "    let x2 = x[.ellipsis, halfDim...]",
      "    return concatenated([-x2, x1], axis: -1)",
      "}"
    ]
  }

  /**
   * Generate a Swift module class
   */
  private genModule(m: ParsedModule): string[] {
    // Route to specialized generators
    if (m.name.includes("RMSNorm")) {
      return this.genRmsNorm(m)
    }
    if (m.name.includes("Embedding") && m.baseClasses.includes("nn.Embedding")) {
      return this.genScaledEmbedding(m)
    }
    if (m.name.includes("Attention")) {
      return this.genAttention(m)
    }
    if (m.name.includes("MLP")) {
      return this.genMlp(m)
    }
    if (m.name.includes("DecoderLayer")) {
      return this.genDecoderLayer(m)
    }
    if (m.name.includes("RotaryEmbedding")) {
      return this.genRotaryEmbedding(m)
    }
    return this.genGenericModule(m)
  }

  /**
   * Generate RMSNorm class
   */
  private genRmsNorm(m: ParsedModule): string[] {
    return [
      `class ${m.swiftName}: Module {`,
      "    let eps: Float",
      "    let weight: MLXArray",
      "    let withScale: Bool",
      "",
      "    init(dimensions: Int, eps: Float = 1e-6, withScale: Bool = true) {",
      "        self.eps = eps",
      "        self.withScale = withScale",
      "        self.weight = withScale ? MLXArray.ones([dimensions]) : MLXArray.ones([dimensions])",
      "    }",
      "",
      "    func callAsFunction(_ x: MLXArray) -> MLXArray {",
      "        let variance = x.pow(2).mean(axis: -1, keepDims: true)",
      "        let normalized = x * rsqrt(variance + eps)",
      "        return withScale ? normalized * weight : normalized",
      "    }",
      "}"
    ]
  }

  /**
   * Generate scaled word embedding
   */
  private genScaledEmbedding(m: ParsedModule): string[] {
    return [
      `class ${m.swiftName}: Embedding {`,
      "    let embedScale: Float",
      "",
      "    init(embeddingCount: Int, dimensions: Int, embedScale: Float = 1.0) {",
      "        self.embedScale = embedScale",
      "        super.init(embeddingCount: embeddingCount, dimensions: dimensions)",
      "    }",
      "",
      "    override func callAsFunction(_ x: MLXArray) -> MLXArray {",
      "        return super.callAsFunction(x) * embedScale",
      "    }",
      "}"
    ]
  }

  /**
   * Generate attention module
   */
  private genAttention(m: ParsedModule): string[] {
    const lines = [`class ${m.swiftName}: Module {`]

    // Module attributes
    for (const attr of m.attributes) {
      if (attr.moduleType === "Array") {
        lines.push(`    var ${attr.swiftName}: [Module] = []`)
      } else {
        lines.push(`    @ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
      }
    }

    // Computed properties
    lines.push(
      "",
      "    let numHeads: Int",
      "    let numKVHeads: Int",
      "    let headDim: Int",
      "    let scale: Float",
      "",
      `    init(_ config: ${this.configClass}) {`,
      "        self.numHeads = config.numAttentionHeads",
      "        self.numKVHeads = config.numKeyValueHeads ?? config.numAttentionHeads",
      "        self.headDim = config.headDim ?? (config.hiddenSize / config.numAttentionHeads)",
      "        if let scalar = config.queryPreAttnScalar {",
      "            self.scale = 1.0 / sqrt(Float(scalar))",
      "        } else {",
      "            self.scale = 1.0 / sqrt(Float(headDim))",
      "        }",
      "",
      "        let hiddenSize = config.hiddenSize",
      "        let kvDim = numKVHeads * headDim",
      "        let qDim = numHeads * headDim"
    )

    // Initialize projections
    const hasAttr = (name: string) => m.attributes.some((a) => a.swiftName === name)

    if (hasAttr("qProj")) {
      lines.push("        self._qProj.wrappedValue = Linear(hiddenSize, qDim, bias: false)")
    }
    if (hasAttr("kProj")) {
      lines.push("        self._kProj.wrappedValue = Linear(hiddenSize, kvDim, bias: false)")
    }
    if (hasAttr("vProj")) {
      lines.push("        self._vProj.wrappedValue = Linear(hiddenSize, kvDim, bias: false)")
    }
    if (hasAttr("oProj")) {
      lines.push("        self._oProj.wrappedValue = Linear(qDim, hiddenSize, bias: false)")
    }

    // Initialize norms
    const rmsNormType = `${this.modelName}RMSNorm`
    if (hasAttr("qNorm")) {
      lines.push(
        `        self._qNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6)`
      )
    }
    if (hasAttr("kNorm")) {
      lines.push(
        `        self._kNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6)`
      )
    }
    if (hasAttr("vNorm")) {
      lines.push(
        `        self._vNorm.wrappedValue = ${rmsNormType}(dimensions: headDim, eps: config.rmsNormEps ?? 1e-6, withScale: false)`
      )
    }

    lines.push(
      "    }",
      "",
      "    func callAsFunction(",
      "        _ x: MLXArray,",
      "        mask: MLXArray? = nil,",
      "        cache: KVCache? = nil",
      "    ) -> MLXArray {",
      "        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))",
      "",
      "        var queries = qProj(x).reshaped([B, L, numHeads, headDim])",
      "        var keys = kProj(x).reshaped([B, L, numKVHeads, headDim])",
      "        var values = vProj(x).reshaped([B, L, numKVHeads, headDim])"
    )

    // Apply norms
    if (hasAttr("qNorm")) {
      lines.push("        queries = qNorm(queries)")
    }
    if (hasAttr("kNorm")) {
      lines.push("        keys = kNorm(keys)")
    }
    if (hasAttr("vNorm")) {
      lines.push("        values = vNorm(values)")
    }

    lines.push(
      "",
      "        queries = queries.transposed(0, 2, 1, 3)",
      "        keys = keys.transposed(0, 2, 1, 3)",
      "        values = values.transposed(0, 2, 1, 3)",
      "",
      "        // KV cache update",
      "        if var cache = cache {",
      "            (keys, values) = cache.update(keys: keys, values: values)",
      "        }",
      "",
      "        // Expand KV heads if needed",
      "        if numKVHeads < numHeads {",
      "            let repeats = numHeads / numKVHeads",
      "            keys = MLXArray.repeated(keys, count: repeats, axis: 1)",
      "            values = MLXArray.repeated(values, count: repeats, axis: 1)",
      "        }",
      "",
      "        var scores = matmul(queries, keys.transposed(0, 1, 3, 2)) * scale",
      "        if let mask = mask {",
      "            scores = scores + mask",
      "        }",
      "        let weights = softmax(scores, axis: -1)",
      "        let output = matmul(weights, values)",
      "            .transposed(0, 2, 1, 3)",
      "            .reshaped([B, L, -1])",
      "",
      "        return oProj(output)",
      "    }",
      "}"
    )

    return lines
  }

  /**
   * Generate MLP module
   */
  private genMlp(m: ParsedModule): string[] {
    const lines = [`class ${m.swiftName}: Module {`]

    for (const attr of m.attributes) {
      lines.push(`    @ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
    }

    lines.push(
      "",
      `    init(_ config: ${this.configClass}) {`,
      "        let hiddenSize = config.hiddenSize",
      "        let intermediateSize = config.intermediateSize"
    )

    const hasAttr = (name: string) => m.attributes.some((a) => a.swiftName === name)

    if (hasAttr("gateProj")) {
      lines.push(
        "        self._gateProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)"
      )
    }
    if (hasAttr("upProj")) {
      lines.push(
        "        self._upProj.wrappedValue = Linear(hiddenSize, intermediateSize, bias: false)"
      )
    }
    if (hasAttr("downProj")) {
      lines.push(
        "        self._downProj.wrappedValue = Linear(intermediateSize, hiddenSize, bias: false)"
      )
    }

    lines.push("    }", "", "    func callAsFunction(_ x: MLXArray) -> MLXArray {")

    if (hasAttr("gateProj")) {
      lines.push("        return downProj(gelu(gateProj(x)) * upProj(x))")
    } else {
      lines.push("        return x")
    }

    lines.push("    }", "}")

    return lines
  }

  /**
   * Generate decoder layer
   */
  private genDecoderLayer(m: ParsedModule): string[] {
    const lines = [`class ${m.swiftName}: Module {`]

    for (const attr of m.attributes) {
      if (attr.moduleType === "Array") {
        continue
      }
      lines.push(`    @ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
    }

    lines.push("", `    init(_ config: ${this.configClass}, layerIdx: Int = 0) {`)

    // Initialize attention
    const attentionAttr = m.attributes.find((a) => a.moduleType?.includes("Attention"))
    if (attentionAttr) {
      lines.push(
        `        self._${attentionAttr.swiftName}.wrappedValue = ${attentionAttr.moduleType}(config)`
      )
    }

    // Initialize MLP
    const mlpAttr = m.attributes.find((a) => a.moduleType?.includes("MLP"))
    if (mlpAttr) {
      lines.push(`        self._${mlpAttr.swiftName}.wrappedValue = ${mlpAttr.moduleType}(config)`)
    }

    // Initialize norms
    const rmsNormType = `${this.modelName}RMSNorm`
    for (const attr of m.attributes) {
      if (attr.moduleType?.includes("Norm")) {
        lines.push(
          `        self._${attr.swiftName}.wrappedValue = ${rmsNormType}(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)`
        )
      }
    }

    // Initialize other modules
    for (const attr of m.attributes) {
      if (attr.moduleType?.includes("LaurelBlock") || attr.moduleType?.includes("AltUp")) {
        lines.push(`        self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      }
    }

    lines.push(
      "    }",
      "",
      "    func callAsFunction(",
      "        _ x: MLXArray,",
      "        mask: MLXArray? = nil,",
      "        cache: KVCache? = nil",
      "    ) -> MLXArray {",
      "        var h = x"
    )

    // Pre-attention norm
    const preAttnNorm = m.attributes.find((a) =>
      ["inputLayernorm", "preAttnNorm", "preFfnNorm"].includes(a.swiftName)
    )
    if (preAttnNorm) {
      lines.push(`        let normed = ${preAttnNorm.swiftName}(h)`)
    } else {
      lines.push("        let normed = h")
    }

    // Attention
    if (attentionAttr) {
      lines.push(
        `        let attnOut = ${attentionAttr.swiftName}(normed, mask: mask, cache: cache)`
      )
      lines.push("        h = h + attnOut")
    }

    // Post-attention norm
    const postAttnNorm = m.attributes.find((a) =>
      ["postAttentionLayernorm", "postAttnNorm"].includes(a.swiftName)
    )
    if (postAttnNorm) {
      lines.push(`        let postNormed = ${postAttnNorm.swiftName}(h)`)
    } else {
      lines.push("        let postNormed = h")
    }

    // MLP
    if (mlpAttr) {
      lines.push(`        let mlpOut = ${mlpAttr.swiftName}(postNormed)`)
      lines.push("        h = h + mlpOut")
    }

    lines.push("", "        return h", "    }", "}")

    return lines
  }

  /**
   * Generate rotary position embedding
   */
  private genRotaryEmbedding(m: ParsedModule): string[] {
    return [
      `class ${m.swiftName}: Module {`,
      "    let dim: Int",
      "    let maxPositionEmbeddings: Int",
      "    let base: Float",
      "",
      `    init(_ config: ${this.configClass}) {`,
      "        self.dim = config.headDim ?? (config.hiddenSize / config.numAttentionHeads)",
      "        self.maxPositionEmbeddings = config.maxPositionEmbeddings ?? 8192",
      "        self.base = config.ropeTheta ?? 10000.0",
      "    }",
      "",
      "    func callAsFunction(_ x: MLXArray, offset: Int = 0) -> (MLXArray, MLXArray) {",
      "        let seqLen = x.dim(1)",
      "        let freqs = MLXArray(stride(from: 0, to: Float(dim), by: 2).map { pow(base, -$0 / Float(dim)) })",
      "        let positions = MLXArray((offset..<(offset + seqLen)).map { Float($0) })",
      "        let angles = positions.expandedDimensions(axis: 1) * freqs.expandedDimensions(axis: 0)",
      "        return (cos(angles), sin(angles))",
      "    }",
      "}"
    ]
  }

  /**
   * Generate a generic module
   */
  private genGenericModule(m: ParsedModule): string[] {
    const lines = [`class ${m.swiftName}: Module {`]

    for (const attr of m.attributes) {
      if (attr.moduleType === "Array") {
        lines.push(`    var ${attr.swiftName}: [Module] = []`)
      } else {
        lines.push(`    @ModuleInfo(key: "${attr.key}") var ${attr.swiftName}: ${attr.moduleType}`)
      }
    }

    lines.push("", `    init(_ config: ${this.configClass}) {`)

    for (const attr of m.attributes) {
      if (attr.moduleType === "Linear") {
        lines.push(
          `        self._${attr.swiftName}.wrappedValue = Linear(config.hiddenSize, config.hiddenSize, bias: false)`
        )
      } else if (attr.moduleType === "Embedding") {
        lines.push(
          `        self._${attr.swiftName}.wrappedValue = Embedding(embeddingCount: config.vocabSize, dimensions: config.hiddenSize)`
        )
      } else if (attr.moduleType?.includes("RMSNorm")) {
        lines.push(
          `        self._${attr.swiftName}.wrappedValue = ${this.modelName}RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)`
        )
      } else if (attr.moduleType?.includes("ScaledWordEmbedding")) {
        lines.push(
          `        self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(`,
          "            embeddingCount: config.vocabSize,",
          "            dimensions: config.hiddenSize,",
          "            embedScale: sqrt(Float(config.hiddenSize))",
          "        )"
        )
      } else if (attr.moduleType?.includes("RotaryEmbedding")) {
        lines.push(`        self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      } else if (this.parsedModules.has(attr.moduleType || "")) {
        lines.push(`        self._${attr.swiftName}.wrappedValue = ${attr.moduleType}(config)`)
      }
    }

    lines.push(
      "    }",
      "",
      "    func callAsFunction(_ x: MLXArray) -> MLXArray {",
      "        return x",
      "    }",
      "}"
    )

    return lines
  }

  /**
   * Generate the top-level model wrapper
   */
  private genModelWrapper(modules: ParsedModule[]): string[] {
    // Find decoder layer name
    const decoderLayer = modules.find((m) => m.name.includes("DecoderLayer"))
    const decoderLayerName = decoderLayer?.name || "Module"

    return [
      `// MARK: - ${this.modelName}ModelInner`,
      "",
      `class ${this.modelName}ModelInner: Module {`,
      `    @ModuleInfo(key: "embed_tokens") var embedTokens: ${this.modelName}TextScaledWordEmbedding`,
      `    @ModuleInfo(key: "layers") var layers: [${decoderLayerName}]`,
      `    @ModuleInfo(key: "norm") var norm: ${this.modelName}RMSNorm`,
      "",
      `    init(_ config: ${this.configClass}) {`,
      `        self._embedTokens.wrappedValue = ${this.modelName}TextScaledWordEmbedding(`,
      "            embeddingCount: config.vocabSize,",
      "            dimensions: config.hiddenSize,",
      "            embedScale: config.hiddenSize > 0 ? sqrt(Float(config.hiddenSize)) : 1.0",
      "        )",
      `        self._layers.wrappedValue = (0..<config.numHiddenLayers).map { ${decoderLayerName}(config, layerIdx: $0) }`,
      `        self._norm.wrappedValue = ${this.modelName}RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps ?? 1e-6)`,
      "    }",
      "",
      "    func callAsFunction(_ inputIds: MLXArray, cache: [KVCache]? = nil) -> MLXArray {",
      "        var h = embedTokens(inputIds)",
      "        for (i, layer) in layers.enumerated() {",
      "            let layerCache = cache?[i]",
      "            h = layer(h, mask: nil, cache: layerCache)",
      "        }",
      "        return norm(h)",
      "    }",
      "}",
      "",
      `// MARK: - ${this.modelName}Model`,
      "",
      `public class ${this.modelName}Model: Module, LLMModel {`,
      "    public let vocabularySize: Int",
      "    public let numLayers: Int",
      "",
      `    @ModuleInfo(key: "model") var model: ${this.modelName}ModelInner`,
      '    @ModuleInfo(key: "lm_head") var lmHead: Linear',
      `    private let config: ${this.configClass}`,
      "",
      `    public init(_ config: ${this.configClass}) {`,
      "        self.config = config",
      "        self.vocabularySize = config.vocabSize",
      "        self.numLayers = config.numHiddenLayers",
      `        self._model.wrappedValue = ${this.modelName}ModelInner(config)`,
      "        self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabSize, bias: false)",
      "    }",
      "",
      "    // LLMModel protocol requirement",
      "    public func callAsFunction(_ inputIds: MLXArray) -> MLXArray {",
      "        let h = model(inputIds, cache: nil)",
      "        return lmHead(h)",
      "    }",
      "",
      "    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {",
      "        var result: [String: MLXArray] = [:]",
      "        for (key, value) in weights {",
      "            var newKey = key",
      '            if newKey.hasPrefix("model.language_model.") {',
      '                newKey = String(newKey.dropFirst("model.language_model.".count))',
      '            } else if newKey.hasPrefix("language_model.") {',
      '                newKey = String(newKey.dropFirst("language_model.".count))',
      "            }",
      "            result[newKey] = value",
      "        }",
      "        return result",
      "    }",
      "}"
    ]
  }
}
