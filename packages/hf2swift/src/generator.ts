/**
 * Swift Code Generator
 *
 * Generates MLX Swift model code from parsed Python modules.
 * Output is formatted with SwiftFormat for consistent style.
 *
 * Production quality: Uses MLXFast, RoPE providers, proper caching,
 * sliding window attention, geluApproximate, clipResidual, etc.
 */

import { execSync } from "node:child_process"
import type { ParsedModule } from "./types.js"
import { toPascal } from "./naming.js"
import { generateConfigFromJson } from "./config.js"

/**
 * Model-specific feature flags
 */
export interface ModelFeatures {
  // RMSNorm style: "gemma" uses (1+weight), "standard" uses weight directly
  rmsNormStyle: "gemma" | "standard"
  // Activation: "gelu" or "geluApproximate" (Gemma uses approximate)
  activation: "gelu" | "geluApproximate" | "silu"
  // Use clipResidual for float16 overflow protection
  useClipResidual: boolean
  // Sliding window attention support
  useSlidingWindow: boolean
  // Default RoPE theta (10000 for most, 1000000 for Gemma3)
  defaultRopeTheta: number
  // Has separate local RoPE theta for sliding window layers
  hasLocalRopeTheta: boolean
  // Gemma-style embedding scaling (multiply by sqrt(hiddenSize))
  useEmbeddingScale: boolean
  // Has Q/K norms before attention
  hasQKNorms: boolean
  // Number of norms per decoder layer (2 for most, 4 for Gemma3)
  normsPerLayer: 2 | 4
}

/**
 * Get default features for a model type
 */
export function getModelFeatures(modelType: string): ModelFeatures {
  const lower = modelType.toLowerCase()

  if (lower.includes("gemma3") || lower.includes("gemma-3")) {
    return {
      rmsNormStyle: "gemma",
      activation: "geluApproximate",
      useClipResidual: true,
      useSlidingWindow: true,
      defaultRopeTheta: 1000000,
      hasLocalRopeTheta: true,
      useEmbeddingScale: true,
      hasQKNorms: true,
      normsPerLayer: 4
    }
  }

  if (lower.includes("qwen")) {
    return {
      rmsNormStyle: "standard",
      activation: "silu",
      useClipResidual: false,
      useSlidingWindow: false,
      defaultRopeTheta: 10000,
      hasLocalRopeTheta: false,
      useEmbeddingScale: false,
      hasQKNorms: false,
      normsPerLayer: 2
    }
  }

  if (lower.includes("llama")) {
    return {
      rmsNormStyle: "standard",
      activation: "silu",
      useClipResidual: false,
      useSlidingWindow: false,
      defaultRopeTheta: 10000,
      hasLocalRopeTheta: false,
      useEmbeddingScale: false,
      hasQKNorms: false,
      normsPerLayer: 2
    }
  }

  if (lower.includes("phi")) {
    return {
      rmsNormStyle: "standard",
      activation: "silu",
      useClipResidual: false,
      useSlidingWindow: false,
      defaultRopeTheta: 10000,
      hasLocalRopeTheta: false,
      useEmbeddingScale: false,
      hasQKNorms: false,
      normsPerLayer: 2
    }
  }

  // Default features
  return {
    rmsNormStyle: "standard",
    activation: "gelu",
    useClipResidual: false,
    useSlidingWindow: false,
    defaultRopeTheta: 10000,
    hasLocalRopeTheta: false,
    useEmbeddingScale: false,
    hasQKNorms: false,
    normsPerLayer: 2
  }
}

/**
 * Format Swift code using swiftformat
 */
export function formatSwift(code: string): string {
  try {
    return execSync("swiftformat stdin", {
      input: code,
      encoding: "utf-8",
      maxBuffer: 10 * 1024 * 1024
    })
  } catch {
    console.warn("Warning: swiftformat not available, returning unformatted code")
    return code
  }
}

/**
 * Swift Model Generator - Production Quality
 */
export class SwiftGenerator {
  private modelName: string
  private configClass: string
  private features: ModelFeatures

  constructor(modelName: string, features?: ModelFeatures) {
    this.modelName = toPascal(modelName)
    this.configClass = `${this.modelName}Configuration`
    this.features = features ?? getModelFeatures(modelName)
  }

  /**
   * Generate complete Swift file
   */
  generate(modules: ParsedModule[], configJson?: Record<string, unknown>): string {
    const parts: string[] = [
      this.genHeader(),
      configJson ? generateConfigFromJson(configJson, this.modelName, this.features) : "",
      this.genRmsNorm(),
      this.genHelpers(),
      this.genAttention(modules),
      this.genMlp(modules),
      this.genDecoderLayer(modules),
      this.genModelInner(modules),
      this.genModel(modules)
    ]

    const code = parts.filter(Boolean).join("\n\n")
    return formatSwift(code)
  }

  private genHeader(): string {
    return `//
//  ${this.modelName}.swift
//  NodeMLXCore
//
//  ${this.modelName} Model implementation (auto-generated by hf2swift).
//  Based on patterns from mlx-lm and mlx-swift-lm.
//

import Foundation
import MLX
import MLXFast
import MLXNN`
  }

  private genHelpers(): string {
    const parts: string[] = ["// MARK: - Utility Functions"]

    // clipResidual helper for float16 overflow protection
    if (this.features.useClipResidual) {
      parts.push(`
/// Clip residual for float16 overflow protection (matching mlx-lm)
private func clipResidual(_ x: MLXArray, _ y: MLXArray) -> MLXArray {
    if x.dtype != .float16 {
        return x + y
    }
    let bound = Float16.greatestFiniteMagnitude
    let sum = (x.asType(.float32) + y.asType(.float32))
    return clip(sum, min: MLXArray(-Float(bound)), max: MLXArray(Float(bound))).asType(.float16)
}`)
    }

    return parts.join("\n")
  }

  private genRmsNorm(): string {
    const name = this.modelName
    const f = this.features

    if (f.rmsNormStyle === "gemma") {
      return `// MARK: - RMS Norm (Gemma style - uses 1+weight scaling)

class ${name}RMSNorm: Module {
    let eps: Float

    @ModuleInfo(key: "weight") var weight: MLXArray

    init(dimensions: Int, eps: Float = 1e-6) {
        self.eps = eps
        // Initialize to zeros - will be (1 + weight) in forward
        self._weight.wrappedValue = MLXArray.zeros([dimensions])
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        // Gemma uses (1 + weight) scaling
        return MLXFast.rmsNorm(x, weight: 1 + weight, eps: eps)
    }
}`
    }

    // Standard RMSNorm
    return `// MARK: - RMS Norm

class ${name}RMSNorm: Module {
    let eps: Float

    @ModuleInfo(key: "weight") var weight: MLXArray

    init(dimensions: Int, eps: Float = 1e-6) {
        self.eps = eps
        self._weight.wrappedValue = MLXArray.ones([dimensions])
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        return MLXFast.rmsNorm(x, weight: weight, eps: eps)
    }
}`
  }

  private genAttention(_modules: ParsedModule[]): string {
    const name = this.modelName
    const f = this.features
    const normType = `${name}RMSNorm`

    // Build Q/K norm declarations if needed
    const qkNormDecl = f.hasQKNorms
      ? `
    @ModuleInfo(key: "q_norm") var qNorm: ${normType}
    @ModuleInfo(key: "k_norm") var kNorm: ${normType}`
      : ""

    const qkNormInit = f.hasQKNorms
      ? `
        self._qNorm.wrappedValue = ${normType}(dimensions: headDim, eps: config.rmsNormEps)
        self._kNorm.wrappedValue = ${normType}(dimensions: headDim, eps: config.rmsNormEps)`
      : ""

    const qkNormApply = f.hasQKNorms
      ? `
        // Apply RMSNorm to Q and K
        queries = qNorm(queries)
        keys = kNorm(keys)`
      : ""

    // RoPE initialization - use provider for sliding window support
    const ropeDecl = f.useSlidingWindow
      ? `let rope: any RoPEProvider
    let isGlobal: Bool
    let slidingWindow: Int?`
      : `let rope: RoPE`

    const ropeInit = f.useSlidingWindow
      ? `
        self.isGlobal = config.isGlobalLayer(layerIdx)
        self.slidingWindow = isGlobal ? nil : config.slidingWindow

        // Different RoPE theta for sliding vs global layers
        let ropeBase = isGlobal ? config.ropeTheta : config.ropeLocalTheta

        // Use initializeRope to handle linear scaling for larger models
        self.rope = initializeRope(
            dims: headDim,
            base: ropeBase,
            traditional: false,
            scalingConfig: isGlobal ? config.ropeScaling : nil,
            maxPositionEmbeddings: config.maxPositionEmbeddings
        )`
      : `
        self.rope = RoPE(dimensions: headDim, traditional: false, base: config.ropeTheta)`

    // Layer index parameter needed for sliding window
    const layerIdxParam = f.useSlidingWindow ? ", layerIdx: Int" : ""

    return `// MARK: - Attention

class ${name}Attention: Module {
    @ModuleInfo(key: "q_proj") var qProj: Linear
    @ModuleInfo(key: "k_proj") var kProj: Linear
    @ModuleInfo(key: "v_proj") var vProj: Linear
    @ModuleInfo(key: "o_proj") var oProj: Linear${qkNormDecl}

    let numHeads: Int
    let numKVHeads: Int
    let headDim: Int
    let scale: Float
    ${ropeDecl}

    init(_ config: ${this.configClass}${layerIdxParam}) {
        self.numHeads = config.numAttentionHeads
        self.numKVHeads = config.numKeyValueHeads
        self.headDim = config.headDim
        self.scale = 1.0 / sqrt(Float(headDim))

        let qDim = numHeads * headDim
        let kvDim = numKVHeads * headDim

        self._qProj.wrappedValue = Linear(config.hiddenSize, qDim, bias: false)
        self._kProj.wrappedValue = Linear(config.hiddenSize, kvDim, bias: false)
        self._vProj.wrappedValue = Linear(config.hiddenSize, kvDim, bias: false)
        self._oProj.wrappedValue = Linear(qDim, config.hiddenSize, bias: false)
${qkNormInit}
${ropeInit}
    }

    func callAsFunction(
        _ hiddenStates: MLXArray,
        mask: MLXFast.ScaledDotProductAttentionMaskMode,
        cache: inout KVCache?
    ) -> MLXArray {
        let (B, L, _) = (hiddenStates.dim(0), hiddenStates.dim(1), hiddenStates.dim(2))

        // Project to Q, K, V and reshape
        var queries = qProj(hiddenStates).reshaped([B, L, numHeads, headDim])
        var keys = kProj(hiddenStates).reshaped([B, L, numKVHeads, headDim])
        var values = vProj(hiddenStates).reshaped([B, L, numKVHeads, headDim])
${qkNormApply}

        // Transpose for attention: [B, heads, L, headDim]
        queries = queries.transposed(0, 2, 1, 3)
        keys = keys.transposed(0, 2, 1, 3)
        values = values.transposed(0, 2, 1, 3)

        // Apply RoPE with cache offset
        let offset = cache?.offset ?? 0
        queries = rope.apply(queries, offset: offset)
        keys = rope.apply(keys, offset: offset)

        // Update cache
        if let c = cache {
            (keys, values) = c.update(keys: keys, values: values)
        }

        // Attention using MLXFast (handles GQA automatically)
        let output = MLXFast.scaledDotProductAttention(
            queries: queries,
            keys: keys,
            values: values,
            scale: scale,
            mask: mask
        )

        // Reshape back: [B, heads, L, headDim] -> [B, L, hidden]
        let outputReshaped = output.transposed(0, 2, 1, 3).reshaped([B, L, -1])

        return oProj(outputReshaped)
    }
}`
  }

  private genMlp(_modules: ParsedModule[]): string {
    const name = this.modelName
    const f = this.features

    const activation =
      f.activation === "geluApproximate"
        ? "geluApproximate"
        : f.activation === "silu"
          ? "silu"
          : "gelu"

    return `// MARK: - MLP

class ${name}MLP: Module {
    @ModuleInfo(key: "gate_proj") var gateProj: Linear
    @ModuleInfo(key: "up_proj") var upProj: Linear
    @ModuleInfo(key: "down_proj") var downProj: Linear

    init(_ config: ${this.configClass}) {
        self._gateProj.wrappedValue = Linear(config.hiddenSize, config.intermediateSize, bias: false)
        self._upProj.wrappedValue = Linear(config.hiddenSize, config.intermediateSize, bias: false)
        self._downProj.wrappedValue = Linear(config.intermediateSize, config.hiddenSize, bias: false)
    }

    func callAsFunction(_ x: MLXArray) -> MLXArray {
        return downProj(${activation}(gateProj(x)) * upProj(x))
    }
}`
  }

  private genDecoderLayer(_modules: ParsedModule[]): string {
    const name = this.modelName
    const f = this.features
    const normType = `${name}RMSNorm`

    // Extra norms for Gemma-style (4 norms per layer)
    const extraNormDecl =
      f.normsPerLayer === 4
        ? `
    @ModuleInfo(key: "pre_feedforward_layernorm") var preFeedforwardLayernorm: ${normType}
    @ModuleInfo(key: "post_feedforward_layernorm") var postFeedforwardLayernorm: ${normType}`
        : ""

    const extraNormInit =
      f.normsPerLayer === 4
        ? `
        self._preFeedforwardLayernorm.wrappedValue = ${normType}(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        self._postFeedforwardLayernorm.wrappedValue = ${normType}(dimensions: config.hiddenSize, eps: config.rmsNormEps)`
        : ""

    // Layer index for sliding window
    const layerIdxParam = f.useSlidingWindow ? ", layerIdx: Int" : ", layerIdx: Int = 0"
    const attnInit = f.useSlidingWindow
      ? `${name}Attention(config, layerIdx: layerIdx)`
      : `${name}Attention(config)`

    // Forward pass body
    let forwardBody: string
    if (f.normsPerLayer === 4) {
      // Gemma-style with 4 norms and post-norms before residual
      forwardBody = `
        // 1. Pre-norm + Self-attention
        let normed = inputLayernorm(hiddenStates)
        let attnOut = selfAttn(normed, mask: mask, cache: &cache)
        let attnNormed = postAttentionLayernorm(attnOut)
        var h = ${f.useClipResidual ? "clipResidual(hiddenStates, attnNormed)" : "hiddenStates + attnNormed"}

        // 2. Pre-norm + MLP
        let mlpIn = preFeedforwardLayernorm(h)
        let mlpOut = mlp(mlpIn)
        let mlpNormed = postFeedforwardLayernorm(mlpOut)
        h = ${f.useClipResidual ? "clipResidual(h, mlpNormed)" : "h + mlpNormed"}

        return h`
    } else {
      // Standard 2-norm style
      forwardBody = `
        // 1. Pre-norm + Self-attention
        let normed = inputLayernorm(hiddenStates)
        let attnOut = selfAttn(normed, mask: mask, cache: &cache)
        var h = hiddenStates + attnOut

        // 2. Pre-norm + MLP
        let mlpNormed = postAttentionLayernorm(h)
        let mlpOut = mlp(mlpNormed)
        h = h + mlpOut

        return h`
    }

    return `// MARK: - Decoder Layer

class ${name}DecoderLayer: Module {
    @ModuleInfo(key: "self_attn") var selfAttn: ${name}Attention
    @ModuleInfo(key: "mlp") var mlp: ${name}MLP
    @ModuleInfo(key: "input_layernorm") var inputLayernorm: ${normType}
    @ModuleInfo(key: "post_attention_layernorm") var postAttentionLayernorm: ${normType}${extraNormDecl}

    init(_ config: ${this.configClass}${layerIdxParam}) {
        self._selfAttn.wrappedValue = ${attnInit}
        self._mlp.wrappedValue = ${name}MLP(config)
        self._inputLayernorm.wrappedValue = ${normType}(dimensions: config.hiddenSize, eps: config.rmsNormEps)
        self._postAttentionLayernorm.wrappedValue = ${normType}(dimensions: config.hiddenSize, eps: config.rmsNormEps)${extraNormInit}
    }

    func callAsFunction(
        _ hiddenStates: MLXArray,
        mask: MLXFast.ScaledDotProductAttentionMaskMode,
        cache: inout KVCache?
    ) -> MLXArray {${forwardBody}
    }
}`
  }

  private genModelInner(_modules: ParsedModule[]): string {
    const name = this.modelName
    const f = this.features
    const normType = `${name}RMSNorm`

    // Embedding with optional sqrt(hiddenSize) scaling
    const embedInit = f.useEmbeddingScale
      ? `Embedding(embeddingCount: config.vocabSize, dimensions: config.hiddenSize)`
      : `Embedding(embeddingCount: config.vocabSize, dimensions: config.hiddenSize)`

    const embedCall = f.useEmbeddingScale
      ? `
        // Get embeddings and scale by sqrt(hiddenSize) - Gemma specific
        var hiddenStates = embedTokens(inputIds)
        let scale = MLXArray(sqrt(Float(hiddenSize)))
        hiddenStates = hiddenStates * scale.asType(hiddenStates.dtype)`
      : `
        var hiddenStates = embedTokens(inputIds)`

    // Sliding window mask handling
    let maskHandling: string
    let layerLoop: string

    if (f.useSlidingWindow) {
      maskHandling = `
        // Create masks following mlx-lm pattern:
        // - Global mask: uses cache from a global layer (pattern - 1)
        // - Sliding mask: uses cache from first layer with sliding window size
        let globalLayerIdx = slidingWindowPattern - 1
        let globalCache = globalLayerIdx < cache.count ? cache[globalLayerIdx] : nil
        let globalMask = createAttentionMask(h: hiddenStates, cache: globalCache, windowSize: nil)

        // Sliding window mask (only if pattern > 1)
        let slidingMask: MLXFast.ScaledDotProductAttentionMaskMode
        if slidingWindowPattern > 1 {
            let firstCache = cache.first ?? nil
            slidingMask = createAttentionMask(h: hiddenStates, cache: firstCache, windowSize: slidingWindow)
        } else {
            slidingMask = globalMask
        }`
      layerLoop = `
        for i in 0..<layers.count {
            // Layer is global if i % pattern == pattern - 1 (matching mlx-lm)
            let isGlobal = (i % slidingWindowPattern) == (slidingWindowPattern - 1)
            let mask = isGlobal ? globalMask : slidingMask
            hiddenStates = layers[i](hiddenStates, mask: mask, cache: &cache[i])
        }`
    } else {
      maskHandling = `
        let mask = createAttentionMask(h: hiddenStates, cache: cache.first ?? nil, windowSize: nil)`
      layerLoop = `
        for i in 0..<layers.count {
            hiddenStates = layers[i](hiddenStates, mask: mask, cache: &cache[i])
        }`
    }

    const extraProps = f.useSlidingWindow
      ? `
    let slidingWindow: Int
    let slidingWindowPattern: Int`
      : ""

    const extraInit = f.useSlidingWindow
      ? `
        self.slidingWindow = config.slidingWindow
        self.slidingWindowPattern = config.slidingWindowPattern`
      : ""

    return `// MARK: - Model Inner

class ${name}ModelInner: Module {
    @ModuleInfo(key: "embed_tokens") var embedTokens: Embedding
    @ModuleInfo(key: "layers") var layers: [${name}DecoderLayer]
    @ModuleInfo(key: "norm") var norm: ${normType}

    let numLayers: Int
    let hiddenSize: Int${extraProps}

    init(_ config: ${this.configClass}) {
        self.numLayers = config.numHiddenLayers
        self.hiddenSize = config.hiddenSize${extraInit}
        self._embedTokens.wrappedValue = ${embedInit}
        self._layers.wrappedValue = (0..<numLayers).map { idx in ${name}DecoderLayer(config, layerIdx: idx) }
        self._norm.wrappedValue = ${normType}(dimensions: config.hiddenSize, eps: config.rmsNormEps)
    }

    func callAsFunction(
        _ inputIds: MLXArray,
        cache: inout [KVCache?]
    ) -> MLXArray {${embedCall}
${maskHandling}
${layerLoop}

        return norm(hiddenStates)
    }
}`
  }

  private genModel(_modules: ParsedModule[]): string {
    const name = this.modelName
    const f = this.features

    // newCache implementation depends on sliding window
    let newCacheImpl: string
    if (f.useSlidingWindow) {
      newCacheImpl = `
    /// Create a new KV cache with appropriate cache types per layer
    /// Following mlx-lm pattern: global layers use KVCacheSimple, others use RotatingKVCache
    public func newCache() -> [KVCache] {
        return (0..<numLayers).map { i in
            // Layer is global if i % pattern == pattern - 1
            let isGlobal = (i % config.slidingWindowPattern) == (config.slidingWindowPattern - 1)
            if isGlobal {
                return KVCacheSimple()
            } else {
                return RotatingKVCache(maxSize: config.slidingWindow, keep: 0)
            }
        }
    }`
    } else {
      newCacheImpl = `
    /// Create a new KV cache
    public func newCache() -> [KVCache] {
        return (0..<numLayers).map { _ in KVCacheSimple() }
    }`
    }

    return `// MARK: - Top-Level Model

public class ${name}Model: Module, LLMModel {
    public let vocabularySize: Int
    public let numLayers: Int
    public let numKVHeads: Int
    public let headDim: Int

    @ModuleInfo(key: "model") var model: ${name}ModelInner
    @ModuleInfo(key: "lm_head") var lmHead: Linear

    private let config: ${this.configClass}

    public var supportsCache: Bool { true }

    public init(_ config: ${this.configClass}) {
        self.config = config
        self.vocabularySize = config.vocabSize
        self.numLayers = config.numHiddenLayers
        self.numKVHeads = config.numKeyValueHeads
        self.headDim = config.headDim

        self._model.wrappedValue = ${name}ModelInner(config)
        self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabSize, bias: false)
    }

    /// Forward pass without cache
    public func callAsFunction(_ inputIds: MLXArray) -> MLXArray {
        var cache: [KVCache?] = Array(repeating: nil, count: numLayers)
        let h = model(inputIds, cache: &cache)
        return lmHead(h)
    }

    /// Forward pass with KV cache for efficient generation
    public func callAsFunction(_ inputIds: MLXArray, cache: inout [KVCache]?) -> MLXArray {
        var layerCaches: [KVCache?]
        if let existingCache = cache {
            layerCaches = existingCache.map { $0 as KVCache? }
        } else {
            layerCaches = Array(repeating: nil, count: numLayers)
        }

        let h = model(inputIds, cache: &layerCaches)

        cache = layerCaches.compactMap { $0 }

        return lmHead(h)
    }
${newCacheImpl}

    /// Sanitize weight keys from HuggingFace format
    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
        var result: [String: MLXArray] = [:]

        for (key, value) in weights {
            var newKey = key

            // VLM format transformations:
            // - language_model.model.X -> model.X (transformer layers)
            // - language_model.lm_head.X -> lm_head.X (output projection)
            // - language_model.X -> X (other components)
            if newKey.hasPrefix("language_model.model.") {
                newKey = "model." + String(newKey.dropFirst("language_model.model.".count))
            } else if newKey.hasPrefix("language_model.lm_head.") {
                newKey = "lm_head." + String(newKey.dropFirst("language_model.lm_head.".count))
            } else if newKey.hasPrefix("language_model.") {
                newKey = String(newKey.dropFirst("language_model.".count))
            }

            // Skip vision/audio tower weights
            if newKey.contains("vision_tower") || newKey.contains("audio_tower") ||
                newKey.contains("multi_modal_projector") {
                continue
            }

            result[newKey] = value
        }

        // Weight tying fallback: copy embed_tokens to lm_head if missing
        if result["lm_head.weight"] == nil {
            for suffix in ["weight", "scales", "biases"] {
                if let embedWeight = result["model.embed_tokens.\\(suffix)"] {
                    result["lm_head.\\(suffix)"] = embedWeight
                }
            }
        }

        return result
    }
}`
  }
}
