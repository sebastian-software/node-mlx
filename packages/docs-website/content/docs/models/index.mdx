---
title: Model Guide
description: Choose the right model for your use case
---

# Model Guide

node-mlx supports a wide range of models from leading AI providers. Use short aliases or full HuggingFace model IDs.

## Recommended Models

### Qwen 3 (Default)

**Best for:** General use, multilingual tasks, balanced quality/speed

```typescript
loadModel("qwen") // Qwen3-4B-Instruct (best balance)
loadModel("qwen-3-0.6b") // Smallest, fastest
loadModel("qwen-3-1.7b") // Small, good quality
```

| Variant       | Parameters | Memory  | Speed     | Best For        |
| ------------- | ---------- | ------- | --------- | --------------- |
| `qwen-3-0.6b` | 600M       | ~1.2 GB | 180 tok/s | Embedded, edge  |
| `qwen-3-1.7b` | 1.7B       | ~3 GB   | 150 tok/s | General tasks   |
| `qwen`        | 4B         | ~5 GB   | 120 tok/s | **Recommended** |

### Phi 4 (Microsoft)

**Best for:** High quality reasoning, coding tasks

```typescript
loadModel("phi") // Phi-4 (default, 14B, highest quality)
loadModel("phi4") // Phi-4 (alias)
```

### Gemma 3 (Google)

**Best for:** Latest architecture, wide range of sizes

```typescript
loadModel("gemma-3") // Gemma-3-1B
loadModel("gemma-3-4b") // Gemma-3-4B
loadModel("gemma-3-12b") // Gemma-3-12B
loadModel("gemma-3-27b") // Gemma-3-27B (largest)
```

### Gemma 3n (Google)

**Best for:** Efficient architecture, good quality/memory ratio

```typescript
loadModel("gemma-3n") // Gemma-3n-E4B (default)
loadModel("gemma-3n-e2b") // Gemma-3n-E2B (smaller)
```

### Llama 4 (Meta)

**Best for:** Advanced reasoning, multilingual, large context

> **Note:** Requires HuggingFace authentication. Run `huggingface-cli login` first.

```typescript
loadModel("llama") // Llama-4-Scout (default)
loadModel("llama-4-scout") // Llama-4-Scout-17B-16E
```

### GPT-OSS (OpenAI)

**Best for:** Large-scale tasks, MoE architecture

```typescript
loadModel("mlx-community/gpt-oss-20b-MXFP4-Q8") // 20B MoE
loadModel("mlx-community/gpt-oss-120b-MXFP4-Q8") // 120B MoE
```

## Using HuggingFace Models

You can use any compatible model from [mlx-community](https://huggingface.co/mlx-community):

```typescript
loadModel("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
loadModel("mlx-community/gemma-3-4b-it-4bit")
loadModel("mlx-community/phi-4-4bit")
```

## Model Quantization

Most models come in two variants:

| Variant   | Memory      | Quality          | Speed        |
| --------- | ----------- | ---------------- | ------------ |
| **bf16**  | ~2× size    | 100% (reference) | Baseline     |
| **4-bit** | ~4× smaller | ~97-99%          | Often faster |

**When to use 4-bit:**

- Limited RAM (8-16 GB)
- Larger models (7B+)
- General conversation, creative writing

**When to use bf16:**

- Math, logic, coding tasks
- Maximum quality critical
- Smaller models where memory isn't a concern

```typescript
// 4-bit: ~4 GB RAM
loadModel("mlx-community/phi-4-4bit")

// bf16: ~28 GB RAM
loadModel("mlx-community/phi-4-bf16")
```

## Supported Architectures

| Architecture | Example Models        | Status          |
| ------------ | --------------------- | --------------- |
| **Qwen2**    | Qwen 2.5              | ✅ Full support |
| **Qwen3**    | Qwen3 0.6B–4B         | ✅ Full support |
| **Llama**    | Llama 4, Mistral      | ✅ Full support |
| **Phi3**     | Phi-4                 | ✅ Full support |
| **Gemma3**   | Gemma 3 (1B–27B)      | ✅ Full support |
| **Gemma3n**  | Gemma 3n E2B/E4B      | ✅ Full support |
| **Mistral3** | Ministral 3 (3B–14B)  | ✅ Full support |
| **SmolLM3**  | SmolLM3 3B            | ✅ Full support |
| **GPT-OSS**  | GPT-OSS 20B/120B MoE  | ✅ Full support |
